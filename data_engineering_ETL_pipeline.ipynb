{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50201c6",
   "metadata": {},
   "source": [
    "# ETL Pipeline (Extract → Transform → Load)\n",
    "\n",
    "This notebook demonstrates the complete ETL pipeline for the Smart Assign MVP:\n",
    "\n",
    "1. Generate synthetic CV Partner–style reports  \n",
    "2. Extract raw CSVs  \n",
    "3. Transform them into a clean relational schema  \n",
    "4. Load into PostgreSQL  \n",
    "5. Perform basic verification  \n",
    "\n",
    "The goal is to demonstrate a clear, testable, step-by-step ETL workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bbfdd",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. Create a Python virtual environment  \n",
    "2. Select the venv kernel  \n",
    "3. Install required dependencies (`pip install -r requirements.txt`)  \n",
    "4. Ensure PostgreSQL is running  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17b802",
   "metadata": {},
   "source": [
    "## Step 0 — Generate Synthetic Data\n",
    "\n",
    "We begin by generating synthetic Flowcase-style CV Partner reports.  \n",
    "These are stored under the `cv_reports/` folder using timestamped folder names.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d5506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ user_report.csv: 500 rows\n",
      "✔ usage_report.csv: 500 rows\n",
      "✔ project_experiences.csv: 1501 rows\n",
      "✔ certifications.csv: 1006 rows\n",
      "✔ courses.csv: 1446 rows\n",
      "✔ languages.csv: 996 rows\n",
      "✔ technologies.csv: 2220 rows\n",
      "✔ key_qualifications.csv: 485 rows\n",
      "✔ educations.csv: 757 rows\n",
      "✔ work_experiences.csv: 1484 rows\n",
      "✔ positions.csv: 1264 rows\n",
      "✔ blogs.csv: 758 rows\n",
      "✔ cv_roles.csv: 1001 rows\n",
      "✔ sc_clearance.csv: 500 rows\n",
      "✔ availability_report.csv: 30000 rows\n",
      "\n",
      "All files written under: /Users/alanabarrett-frew/Desktop/Module  4/Assignment/ETL Pipeline/preprocessing/ETL_pipeline/cv_reports/Q42025\n"
     ]
    }
   ],
   "source": [
    "import make_fake_flowcase_reports\n",
    "\n",
    "make_fake_flowcase_reports.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f28053",
   "metadata": {},
   "source": [
    "## Run all Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1373d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import date\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514123e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_step(title):\n",
    "    print(f\"\\n{'='*20} {title} {'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce0a69",
   "metadata": {},
   "source": [
    "# Step 1 — Extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2036190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_report_folder(base_folder=\"cv_reports\"):\n",
    "    print_step(\"Finding the latest report folder\")\n",
    "\n",
    "    report_folders = [f for f in Path(base_folder).iterdir() if f.is_dir()]\n",
    "    if not report_folders:\n",
    "        raise FileNotFoundError(f\"No report folders found in {base_folder}\")\n",
    "\n",
    "    latest = sorted(report_folders, key=lambda f: f.name)[-1]\n",
    "\n",
    "    print(f\"Found {len(report_folders)} folders.\")\n",
    "    print(f\"Latest folder: {latest.name}\")\n",
    "\n",
    "    return latest\n",
    "\n",
    "\n",
    "def find_latest_quarterly_report_folder(base_folder=\"cv_reports\"):\n",
    "    print_step(\"Finding the latest quarterly report folder\")\n",
    "\n",
    "    pattern = re.compile(r\"Q[1-4]\\d{4}\")\n",
    "    report_folders = [\n",
    "        f for f in Path(base_folder).iterdir()\n",
    "        if f.is_dir() and pattern.match(f.name)\n",
    "    ]\n",
    "    if not report_folders:\n",
    "        raise FileNotFoundError(f\"No quarterly report folders found in {base_folder}.\")\n",
    "\n",
    "    names = sorted([f.name for f in report_folders])\n",
    "    print(\"Quarterly folders found:\", names)\n",
    "\n",
    "    latest_folder = sorted(report_folders, key=lambda folder: folder.name)[-1]\n",
    "    print(\"Using latest quarterly folder:\", latest_folder.name)\n",
    "\n",
    "    return latest_folder\n",
    "\n",
    "\n",
    "def load_csv_files_from_folder(report_folder):\n",
    "    print_step(f\"Loading CSV files from {report_folder}\")\n",
    "\n",
    "    csv_files = list(Path(report_folder).glob(\"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files.\")\n",
    "    dataframes = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dataframes[csv_file.name] = df\n",
    "            print(f\"  Loaded {csv_file.name} -> {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Failed to read {csv_file.name}: {e}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def extract(settings):\n",
    "    \"\"\"\n",
    "    Finds the latest quarterly report folder and loads all CSVs as DataFrames.\n",
    "    Returns an object with .data_dir and dict-like data (keyed by filename).\n",
    "    \"\"\"\n",
    "    data_source = settings.get(\"data_source\", \"fake\")\n",
    "\n",
    "    if data_source == \"real\":\n",
    "        print(\"[extract] Real data mode selected, but not implemented.\")\n",
    "        return type(\"ExtractResult\", (), {\"data_dir\": None})()\n",
    "\n",
    "    # Fake data path\n",
    "    base_folder = settings.get(\"base_folder\", \"cv_reports\")\n",
    "    data_dir = find_latest_quarterly_report_folder(base_folder)\n",
    "    data = load_csv_files_from_folder(data_dir)\n",
    "\n",
    "    class ExtractResult(dict):\n",
    "        pass\n",
    "\n",
    "    result = ExtractResult(data)\n",
    "    result.data_dir = data_dir\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad32481",
   "metadata": {},
   "source": [
    "## Step 1.1 — Locate the latest report folder\n",
    "\n",
    "This identifies the most recent synthetic export under `cv_reports/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017eba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 1.1: Locate latest report folder ====================\n",
      "\n",
      "==================== Finding the latest report folder ====================\n",
      "Found 1 folders.\n",
      "Latest folder: Q42025\n",
      "✅ Using folder: cv_reports/Q42025\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 1.1: Locate latest report folder\")\n",
    "\n",
    "latest_folder = find_latest_report_folder(\"cv_reports\")\n",
    "print(f\"✅ Using folder: {latest_folder}\")\n",
    "\n",
    "# Tiny test: does it actually exist?\n",
    "assert latest_folder.exists(), \"Latest folder path does not exist on disk!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a88e7",
   "metadata": {},
   "source": [
    "## Step 1.2 — Load all CSV files from the latest report\n",
    "\n",
    "We load all CSVs inside the selected folder into pandas DataFrames and perform a\n",
    "small sanity check to ensure core files are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30eb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 1.2: Load CSV files from latest folder ====================\n",
      "\n",
      "==================== Loading CSV files from cv_reports/Q42025 ====================\n",
      "Found 15 CSV files.\n",
      "  Loaded certifications.csv -> (1006, 22)\n",
      "  Loaded project_experiences.csv -> (1501, 45)\n",
      "  Loaded blogs.csv -> (758, 21)\n",
      "  Loaded availability_report.csv -> (30000, 7)\n",
      "  Loaded cv_roles.csv -> (1001, 19)\n",
      "  Loaded work_experiences.csv -> (1484, 26)\n",
      "  Loaded educations.csv -> (757, 27)\n",
      "  Loaded user_report.csv -> (500, 26)\n",
      "  Loaded courses.csv -> (1446, 26)\n",
      "  Loaded key_qualifications.csv -> (485, 21)\n",
      "  Loaded positions.csv -> (1264, 23)\n",
      "  Loaded technologies.csv -> (2220, 20)\n",
      "  Loaded sc_clearance.csv -> (500, 9)\n",
      "  Loaded languages.csv -> (996, 22)\n",
      "  Loaded usage_report.csv -> (500, 51)\n",
      "\n",
      "Summary of loaded files:\n",
      " - certifications.csv             (1006, 22)\n",
      " - project_experiences.csv        (1501, 45)\n",
      " - blogs.csv                      (758, 21)\n",
      " - availability_report.csv        (30000, 7)\n",
      " - cv_roles.csv                   (1001, 19)\n",
      " - work_experiences.csv           (1484, 26)\n",
      " - educations.csv                 (757, 27)\n",
      " - user_report.csv                (500, 26)\n",
      " - courses.csv                    (1446, 26)\n",
      " - key_qualifications.csv         (485, 21)\n",
      " - positions.csv                  (1264, 23)\n",
      " - technologies.csv               (2220, 20)\n",
      " - sc_clearance.csv               (500, 9)\n",
      " - languages.csv                  (996, 22)\n",
      " - usage_report.csv               (500, 51)\n",
      "\n",
      "Expected core files: ['user_report.csv', 'project_experiences.csv', 'work_experiences.csv']\n",
      "Missing: []\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 1.2: Load CSV files from latest folder\")\n",
    "\n",
    "raw_frames = load_csv_files_from_folder(latest_folder)\n",
    "\n",
    "print(\"\\nSummary of loaded files:\")\n",
    "for name, df in raw_frames.items():\n",
    "    print(f\" - {name:30} {df.shape}\")\n",
    "\n",
    "# Simple check: do we have some expected core CSVs?\n",
    "expected_files = [\n",
    "    \"user_report.csv\",\n",
    "    \"project_experiences.csv\",\n",
    "    \"work_experiences.csv\",\n",
    "]\n",
    "missing = [f for f in expected_files if f not in raw_frames]\n",
    "\n",
    "print(\"\\nExpected core files:\", expected_files)\n",
    "print(\"Missing:\", missing)\n",
    "\n",
    "assert not missing, \"One or more expected CSVs are missing!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bae474",
   "metadata": {},
   "source": [
    "# Step 2 — Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93e221",
   "metadata": {},
   "source": [
    "## Step 2.1 — Transform helpers\n",
    "\n",
    "These helpers normalise multilang fields, dates, and define the `TransformResult`\n",
    "dataclass for returning a clean set of tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d4ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformResult:\n",
    "    users_df: pd.DataFrame | None = None\n",
    "    cvs_df: pd.DataFrame | None = None\n",
    "    technologies_df: pd.DataFrame | None = None\n",
    "    languages_df: pd.DataFrame | None = None\n",
    "    project_experiences_df: pd.DataFrame | None = None\n",
    "    work_experiences_df: pd.DataFrame | None = None\n",
    "    certifications_df: pd.DataFrame | None = None\n",
    "    courses_df: pd.DataFrame | None = None\n",
    "    educations_df: pd.DataFrame | None = None\n",
    "    positions_df: pd.DataFrame | None = None\n",
    "    blogs_df: pd.DataFrame | None = None\n",
    "    cv_roles_df: pd.DataFrame | None = None\n",
    "    key_qualifications_df: pd.DataFrame | None = None\n",
    "    sc_clearance_df: pd.DataFrame | None = None\n",
    "    availability_df: pd.DataFrame | None = None\n",
    "\n",
    "def parse_multilang(pipe: object) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a single pipe string like 'int:Text|no:Tekst' into a dict.\n",
    "    Anything non-string or blank -> {}.\n",
    "    \"\"\"\n",
    "    if not isinstance(pipe, str) or not pipe.strip():\n",
    "        return {}\n",
    "    out = {}\n",
    "    for part in pipe.split(\"|\"):\n",
    "        if \":\" in part:\n",
    "            k, v = part.split(\":\", 1)\n",
    "            k, v = k.strip(), v.strip()\n",
    "            if k and v:\n",
    "                out[k] = v\n",
    "    return out\n",
    "\n",
    "def to_iso_date(s: object) -> str | None:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # If it looks like ISO (yyyy-mm-dd), parse straight without dayfirst\n",
    "    if \"-\" in s and len(s.split(\"-\")[0]) == 4:\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    else:\n",
    "        dt = pd.to_datetime(s, dayfirst=True, errors=\"coerce\")\n",
    "    return None if pd.isna(dt) else dt.date().isoformat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba73a74",
   "metadata": {},
   "source": [
    "## Step 2.2 — Core transform logic\n",
    "\n",
    "The `transform()` function builds clean tables for Users, CVs, skills, \n",
    "experiences, and other CV Partner sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f5fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data) -> TransformResult:\n",
    "    # Core extracts\n",
    "    users = data.get(\"user_report.csv\", pd.DataFrame()).copy()\n",
    "    usage = data.get(\"usage_report.csv\", pd.DataFrame()).copy()\n",
    "\n",
    "    # Parse user name as dict\n",
    "    if not users.empty and \"Name (multilang)\" in users.columns:\n",
    "        users[\"Name (multilang)\"] = users[\"Name (multilang)\"].map(parse_multilang)\n",
    "    else:\n",
    "        users[\"Name (multilang)\"] = [{}] * len(users)\n",
    "\n",
    "    # nationality comes from usage_report: \"Nationality (#{lang})\" is a single pipe string\n",
    "    if not usage.empty and \"Nationality (#{lang})\" in usage.columns:\n",
    "        nat_map = {\n",
    "            str(r[\"CV Partner User ID\"]): parse_multilang(r[\"Nationality (#{lang})\"])\n",
    "            for _, r in usage.iterrows()\n",
    "            if \"CV Partner User ID\" in r and pd.notna(r[\"CV Partner User ID\"])\n",
    "        }\n",
    "        users[\"nationality_multilang\"] = users[\"CV Partner User ID\"].map(\n",
    "            lambda uid: nat_map.get(str(uid), {})\n",
    "        )\n",
    "    else:\n",
    "        users[\"nationality_multilang\"] = [{}] * len(users)\n",
    "\n",
    "    # Build CV rows: your user_report already has one row per CV\n",
    "    cvs = users.copy()\n",
    "    if \"Title (#{lang})\" in users.columns:\n",
    "        cvs[\"title_multilang\"] = users[\"Title (#{lang})\"].map(parse_multilang)\n",
    "    else:\n",
    "        cvs[\"title_multilang\"] = [{}] * len(cvs)\n",
    "\n",
    "    # Carry seniority columns from user_report -> cvs_df\n",
    "    def _num(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    cvs[\"sfia_level\"] = users.get(\"SFIA Level\", pd.Series([None]*len(users))).map(_num)\n",
    "    cvs[\"cpd_level\"]  = users.get(\"CPD Level\",  pd.Series([None]*len(users))).map(_num)\n",
    "    cvs[\"cpd_band\"]   = users.get(\"CPD Band\",   pd.Series([None]*len(users))).astype(\"string\").where(lambda s: s.notna(), None)\n",
    "    cvs[\"cpd_label\"]  = users.get(\"CPD Label\",  pd.Series([None]*len(users))).astype(\"string\").where(lambda s: s.notna(), None)\n",
    "\n",
    "\n",
    "    # Optional extras (pass through + light cleanup)\n",
    "    sc_clearance = data.get(\"sc_clearance.csv\", pd.DataFrame()).copy()\n",
    "    if not sc_clearance.empty:\n",
    "        for col in (\"Valid From\", \"Valid To\"):\n",
    "            if col in sc_clearance.columns:\n",
    "                sc_clearance[col] = sc_clearance[col].map(to_iso_date)\n",
    "\n",
    "    availability = data.get(\"availability_report.csv\", pd.DataFrame()).copy()\n",
    "    if not availability.empty and \"Date\" in availability.columns:\n",
    "        availability[\"Date\"] = availability[\"Date\"].map(to_iso_date)\n",
    "\n",
    "    return TransformResult(\n",
    "        users_df=users if not users.empty else pd.DataFrame(),\n",
    "        cvs_df=cvs if not cvs.empty else pd.DataFrame(),\n",
    "        technologies_df=data.get(\"technologies.csv\"),\n",
    "        languages_df=data.get(\"languages.csv\"),\n",
    "        project_experiences_df=data.get(\"project_experiences.csv\"),\n",
    "        work_experiences_df=data.get(\"work_experiences.csv\"),\n",
    "        certifications_df=data.get(\"certifications.csv\"),\n",
    "        courses_df=data.get(\"courses.csv\"),\n",
    "        educations_df=data.get(\"educations.csv\"),\n",
    "        positions_df=data.get(\"positions.csv\"),\n",
    "        blogs_df=data.get(\"blogs.csv\"),\n",
    "        cv_roles_df=data.get(\"cv_roles.csv\"),\n",
    "        key_qualifications_df=data.get(\"key_qualifications.csv\"),\n",
    "        sc_clearance_df=sc_clearance if not sc_clearance.empty else pd.DataFrame(),\n",
    "        availability_df=availability if not availability.empty else pd.DataFrame(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55720d32",
   "metadata": {},
   "source": [
    "## Step 2.3 — Run transform and perform tests\n",
    "\n",
    "We now execute `transform(raw_data)` and validate:\n",
    "\n",
    "- Row counts  \n",
    "- Key identifiers  \n",
    "- Data integrity (e.g., CV count aligns with user count)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297eed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 2.3: Reload CSVs for transform demo ====================\n",
      "\n",
      "==================== Loading CSV files from cv_reports/Q42025 ====================\n",
      "Found 15 CSV files.\n",
      "  Loaded certifications.csv -> (1006, 22)\n",
      "  Loaded project_experiences.csv -> (1501, 45)\n",
      "  Loaded blogs.csv -> (758, 21)\n",
      "  Loaded availability_report.csv -> (30000, 7)\n",
      "  Loaded cv_roles.csv -> (1001, 19)\n",
      "  Loaded work_experiences.csv -> (1484, 26)\n",
      "  Loaded educations.csv -> (757, 27)\n",
      "  Loaded user_report.csv -> (500, 26)\n",
      "  Loaded courses.csv -> (1446, 26)\n",
      "  Loaded key_qualifications.csv -> (485, 21)\n",
      "  Loaded positions.csv -> (1264, 23)\n",
      "  Loaded technologies.csv -> (2220, 20)\n",
      "  Loaded sc_clearance.csv -> (500, 9)\n",
      "  Loaded languages.csv -> (996, 22)\n",
      "  Loaded usage_report.csv -> (500, 51)\n",
      "\n",
      "==================== Step 2.3a: Run transform() on extracted data ====================\n",
      "\n",
      "Transformed tables (row counts):\n",
      " - users_df                    500 rows\n",
      " - cvs_df                      500 rows\n",
      " - technologies_df            2220 rows\n",
      " - languages_df                996 rows\n",
      " - project_experiences_df     1501 rows\n",
      " - work_experiences_df        1484 rows\n",
      "\n",
      "Sample: users_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Name (multilang)</th>\n",
       "      <th>Title (#{lang})</th>\n",
       "      <th>Email</th>\n",
       "      <th>UPN</th>\n",
       "      <th>External User ID</th>\n",
       "      <th>CV Partner User ID</th>\n",
       "      <th>CV Partner CV ID</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Landline</th>\n",
       "      <th>...</th>\n",
       "      <th>Years since first work experience</th>\n",
       "      <th>Access roles</th>\n",
       "      <th>Has profile image</th>\n",
       "      <th>Owns a reference project</th>\n",
       "      <th>Read and understood privacy notice</th>\n",
       "      <th>SFIA Level</th>\n",
       "      <th>CPD Level</th>\n",
       "      <th>CPD Band</th>\n",
       "      <th>CPD Label</th>\n",
       "      <th>nationality_multilang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Johnson</td>\n",
       "      <td>{'int': 'Danielle Johnson'}</td>\n",
       "      <td>int:Principal C# Developer</td>\n",
       "      <td>danielle.johnson@mail.test</td>\n",
       "      <td>daniellejohnson</td>\n",
       "      <td>ext_0d6f2ab3</td>\n",
       "      <td>0d6f2ab3</td>\n",
       "      <td>cv_0d6f2ab3</td>\n",
       "      <td>958-350-6431</td>\n",
       "      <td>+1-539-500-5329x31839</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>User</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Norwegian'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joshua Walker</td>\n",
       "      <td>{'int': 'Joshua Walker'}</td>\n",
       "      <td>int:Principal Data Engineer</td>\n",
       "      <td>joshua.walker@mail.test</td>\n",
       "      <td>joshuawalker</td>\n",
       "      <td>ext_478ff8ec</td>\n",
       "      <td>478ff8ec</td>\n",
       "      <td>cv_478ff8ec</td>\n",
       "      <td>729-504-2284x21020</td>\n",
       "      <td>001-350-324-0268</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>User</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Swedish'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jill Rhodes</td>\n",
       "      <td>{'int': 'Jill Rhodes'}</td>\n",
       "      <td>int:Senior C# Developer</td>\n",
       "      <td>jill.rhodes@example.org</td>\n",
       "      <td>jillrhodes</td>\n",
       "      <td>ext_418230c4</td>\n",
       "      <td>418230c4</td>\n",
       "      <td>cv_418230c4</td>\n",
       "      <td>975.289.1783x9084</td>\n",
       "      <td>7764617711</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>User</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>{'int': 'British'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patricia Miller</td>\n",
       "      <td>{'int': 'Patricia Miller'}</td>\n",
       "      <td>int:Principal Analytics Engineer</td>\n",
       "      <td>patricia.miller@mail.test</td>\n",
       "      <td>patriciamiller</td>\n",
       "      <td>ext_8cadb34a</td>\n",
       "      <td>8cadb34a</td>\n",
       "      <td>cv_8cadb34a</td>\n",
       "      <td>624-999-8569</td>\n",
       "      <td>896.311.8367x36576</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>User</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Polish'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>{'int': 'Robert Johnson'}</td>\n",
       "      <td>int:Associate ML Engineer</td>\n",
       "      <td>robert.johnson@mail.test</td>\n",
       "      <td>robertjohnson</td>\n",
       "      <td>ext_f4ac20e3</td>\n",
       "      <td>f4ac20e3</td>\n",
       "      <td>cv_f4ac20e3</td>\n",
       "      <td>465-245-2711x11615</td>\n",
       "      <td>001-688-651-6560</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Country Manager</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>{'int': 'Danish'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name             Name (multilang)  \\\n",
       "0  Danielle Johnson  {'int': 'Danielle Johnson'}   \n",
       "1     Joshua Walker     {'int': 'Joshua Walker'}   \n",
       "2       Jill Rhodes       {'int': 'Jill Rhodes'}   \n",
       "3   Patricia Miller   {'int': 'Patricia Miller'}   \n",
       "4    Robert Johnson    {'int': 'Robert Johnson'}   \n",
       "\n",
       "                    Title (#{lang})                       Email  \\\n",
       "0        int:Principal C# Developer  danielle.johnson@mail.test   \n",
       "1       int:Principal Data Engineer     joshua.walker@mail.test   \n",
       "2           int:Senior C# Developer     jill.rhodes@example.org   \n",
       "3  int:Principal Analytics Engineer   patricia.miller@mail.test   \n",
       "4         int:Associate ML Engineer    robert.johnson@mail.test   \n",
       "\n",
       "               UPN External User ID CV Partner User ID CV Partner CV ID  \\\n",
       "0  daniellejohnson     ext_0d6f2ab3           0d6f2ab3      cv_0d6f2ab3   \n",
       "1     joshuawalker     ext_478ff8ec           478ff8ec      cv_478ff8ec   \n",
       "2       jillrhodes     ext_418230c4           418230c4      cv_418230c4   \n",
       "3   patriciamiller     ext_8cadb34a           8cadb34a      cv_8cadb34a   \n",
       "4    robertjohnson     ext_f4ac20e3           f4ac20e3      cv_f4ac20e3   \n",
       "\n",
       "         Phone Number               Landline  ...  \\\n",
       "0        958-350-6431  +1-539-500-5329x31839  ...   \n",
       "1  729-504-2284x21020       001-350-324-0268  ...   \n",
       "2   975.289.1783x9084             7764617711  ...   \n",
       "3        624-999-8569     896.311.8367x36576  ...   \n",
       "4  465-245-2711x11615       001-688-651-6560  ...   \n",
       "\n",
       "   Years since first work experience     Access roles Has profile image  \\\n",
       "0                                  5             User              True   \n",
       "1                                 12             User             False   \n",
       "2                                  9             User              True   \n",
       "3                                 18             User             False   \n",
       "4                                 10  Country Manager              True   \n",
       "\n",
       "  Owns a reference project Read and understood privacy notice SFIA Level  \\\n",
       "0                    False                               True          5   \n",
       "1                    False                              False          5   \n",
       "2                    False                               True          4   \n",
       "3                    False                               True          5   \n",
       "4                     True                              False          2   \n",
       "\n",
       "   CPD Level  CPD Band CPD Label  nationality_multilang  \n",
       "0          3         L     CPD3L   {'int': 'Norwegian'}  \n",
       "1          3         L     CPD3L     {'int': 'Swedish'}  \n",
       "2          3         E     CPD3E     {'int': 'British'}  \n",
       "3          3         L     CPD3L      {'int': 'Polish'}  \n",
       "4          1         E     CPD1E      {'int': 'Danish'}  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample: cvs_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Name (multilang)</th>\n",
       "      <th>Title (#{lang})</th>\n",
       "      <th>Email</th>\n",
       "      <th>UPN</th>\n",
       "      <th>External User ID</th>\n",
       "      <th>CV Partner User ID</th>\n",
       "      <th>CV Partner CV ID</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Landline</th>\n",
       "      <th>...</th>\n",
       "      <th>SFIA Level</th>\n",
       "      <th>CPD Level</th>\n",
       "      <th>CPD Band</th>\n",
       "      <th>CPD Label</th>\n",
       "      <th>nationality_multilang</th>\n",
       "      <th>title_multilang</th>\n",
       "      <th>sfia_level</th>\n",
       "      <th>cpd_level</th>\n",
       "      <th>cpd_band</th>\n",
       "      <th>cpd_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Johnson</td>\n",
       "      <td>{'int': 'Danielle Johnson'}</td>\n",
       "      <td>int:Principal C# Developer</td>\n",
       "      <td>danielle.johnson@mail.test</td>\n",
       "      <td>daniellejohnson</td>\n",
       "      <td>ext_0d6f2ab3</td>\n",
       "      <td>0d6f2ab3</td>\n",
       "      <td>cv_0d6f2ab3</td>\n",
       "      <td>958-350-6431</td>\n",
       "      <td>+1-539-500-5329x31839</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Norwegian'}</td>\n",
       "      <td>{'int': 'Principal C# Developer'}</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joshua Walker</td>\n",
       "      <td>{'int': 'Joshua Walker'}</td>\n",
       "      <td>int:Principal Data Engineer</td>\n",
       "      <td>joshua.walker@mail.test</td>\n",
       "      <td>joshuawalker</td>\n",
       "      <td>ext_478ff8ec</td>\n",
       "      <td>478ff8ec</td>\n",
       "      <td>cv_478ff8ec</td>\n",
       "      <td>729-504-2284x21020</td>\n",
       "      <td>001-350-324-0268</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Swedish'}</td>\n",
       "      <td>{'int': 'Principal Data Engineer'}</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jill Rhodes</td>\n",
       "      <td>{'int': 'Jill Rhodes'}</td>\n",
       "      <td>int:Senior C# Developer</td>\n",
       "      <td>jill.rhodes@example.org</td>\n",
       "      <td>jillrhodes</td>\n",
       "      <td>ext_418230c4</td>\n",
       "      <td>418230c4</td>\n",
       "      <td>cv_418230c4</td>\n",
       "      <td>975.289.1783x9084</td>\n",
       "      <td>7764617711</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>{'int': 'British'}</td>\n",
       "      <td>{'int': 'Senior C# Developer'}</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD3E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patricia Miller</td>\n",
       "      <td>{'int': 'Patricia Miller'}</td>\n",
       "      <td>int:Principal Analytics Engineer</td>\n",
       "      <td>patricia.miller@mail.test</td>\n",
       "      <td>patriciamiller</td>\n",
       "      <td>ext_8cadb34a</td>\n",
       "      <td>8cadb34a</td>\n",
       "      <td>cv_8cadb34a</td>\n",
       "      <td>624-999-8569</td>\n",
       "      <td>896.311.8367x36576</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Polish'}</td>\n",
       "      <td>{'int': 'Principal Analytics Engineer'}</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>{'int': 'Robert Johnson'}</td>\n",
       "      <td>int:Associate ML Engineer</td>\n",
       "      <td>robert.johnson@mail.test</td>\n",
       "      <td>robertjohnson</td>\n",
       "      <td>ext_f4ac20e3</td>\n",
       "      <td>f4ac20e3</td>\n",
       "      <td>cv_f4ac20e3</td>\n",
       "      <td>465-245-2711x11615</td>\n",
       "      <td>001-688-651-6560</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>{'int': 'Danish'}</td>\n",
       "      <td>{'int': 'Associate ML Engineer'}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD1E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name             Name (multilang)  \\\n",
       "0  Danielle Johnson  {'int': 'Danielle Johnson'}   \n",
       "1     Joshua Walker     {'int': 'Joshua Walker'}   \n",
       "2       Jill Rhodes       {'int': 'Jill Rhodes'}   \n",
       "3   Patricia Miller   {'int': 'Patricia Miller'}   \n",
       "4    Robert Johnson    {'int': 'Robert Johnson'}   \n",
       "\n",
       "                    Title (#{lang})                       Email  \\\n",
       "0        int:Principal C# Developer  danielle.johnson@mail.test   \n",
       "1       int:Principal Data Engineer     joshua.walker@mail.test   \n",
       "2           int:Senior C# Developer     jill.rhodes@example.org   \n",
       "3  int:Principal Analytics Engineer   patricia.miller@mail.test   \n",
       "4         int:Associate ML Engineer    robert.johnson@mail.test   \n",
       "\n",
       "               UPN External User ID CV Partner User ID CV Partner CV ID  \\\n",
       "0  daniellejohnson     ext_0d6f2ab3           0d6f2ab3      cv_0d6f2ab3   \n",
       "1     joshuawalker     ext_478ff8ec           478ff8ec      cv_478ff8ec   \n",
       "2       jillrhodes     ext_418230c4           418230c4      cv_418230c4   \n",
       "3   patriciamiller     ext_8cadb34a           8cadb34a      cv_8cadb34a   \n",
       "4    robertjohnson     ext_f4ac20e3           f4ac20e3      cv_f4ac20e3   \n",
       "\n",
       "         Phone Number               Landline  ...  SFIA Level CPD Level  \\\n",
       "0        958-350-6431  +1-539-500-5329x31839  ...           5         3   \n",
       "1  729-504-2284x21020       001-350-324-0268  ...           5         3   \n",
       "2   975.289.1783x9084             7764617711  ...           4         3   \n",
       "3        624-999-8569     896.311.8367x36576  ...           5         3   \n",
       "4  465-245-2711x11615       001-688-651-6560  ...           2         1   \n",
       "\n",
       "  CPD Band CPD Label nationality_multilang  \\\n",
       "0        L     CPD3L  {'int': 'Norwegian'}   \n",
       "1        L     CPD3L    {'int': 'Swedish'}   \n",
       "2        E     CPD3E    {'int': 'British'}   \n",
       "3        L     CPD3L     {'int': 'Polish'}   \n",
       "4        E     CPD1E     {'int': 'Danish'}   \n",
       "\n",
       "                           title_multilang  sfia_level  cpd_level cpd_band  \\\n",
       "0        {'int': 'Principal C# Developer'}           5          3        L   \n",
       "1       {'int': 'Principal Data Engineer'}           5          3        L   \n",
       "2           {'int': 'Senior C# Developer'}           4          3        E   \n",
       "3  {'int': 'Principal Analytics Engineer'}           5          3        L   \n",
       "4         {'int': 'Associate ML Engineer'}           2          1        E   \n",
       "\n",
       "   cpd_label  \n",
       "0      CPD3L  \n",
       "1      CPD3L  \n",
       "2      CPD3E  \n",
       "3      CPD3L  \n",
       "4      CPD1E  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 2.3b: Basic data quality checks on transform output ====================\n",
      "✅ Transform checks passed for users_df and cvs_df.\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 2.3: Reload CSVs for transform demo\")\n",
    "\n",
    "raw_data = load_csv_files_from_folder(latest_folder)\n",
    "\n",
    "print_step(\"Step 2.3a: Run transform() on extracted data\")\n",
    "\n",
    "# Use the raw data dict from load_csv_files_from_folder(...)\n",
    "tr = transform(raw_data)\n",
    "\n",
    "# Basic shape summary\n",
    "print(\"\\nTransformed tables (row counts):\")\n",
    "for name in [\n",
    "    \"users_df\", \"cvs_df\", \"technologies_df\", \"languages_df\",\n",
    "    \"project_experiences_df\", \"work_experiences_df\",\n",
    "]:\n",
    "    df = getattr(tr, name)\n",
    "    if df is not None:\n",
    "        print(f\" - {name:25} {len(df):5d} rows\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nSample: users_df\")\n",
    "display(tr.users_df.head())\n",
    "\n",
    "print(\"\\nSample: cvs_df\")\n",
    "display(tr.cvs_df.head())\n",
    "\n",
    "print_step(\"Step 2.3b: Basic data quality checks on transform output\")\n",
    "\n",
    "users_df = tr.users_df\n",
    "cvs_df = tr.cvs_df\n",
    "\n",
    "# 1) Ensure we actually have users\n",
    "assert not users_df.empty, \"users_df is unexpectedly empty after transform!\"\n",
    "\n",
    "# 2) Key identifier should exist & not be all null\n",
    "assert \"CV Partner User ID\" in users_df.columns, \"Missing CV Partner User ID column in users_df\"\n",
    "assert users_df[\"CV Partner User ID\"].notna().any(), \"All user IDs are null!\"\n",
    "\n",
    "# 3) Same number of rows in users_df and cvs_df (since user_report is 1 CV per row)\n",
    "assert len(users_df) == len(cvs_df), \"users_df and cvs_df row counts differ!\"\n",
    "\n",
    "print(\"✅ Transform checks passed for users_df and cvs_df.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377b6c6",
   "metadata": {},
   "source": [
    "# Step 3 — Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae8579",
   "metadata": {},
   "source": [
    "## Step 3.1 — Load helpers\n",
    "\n",
    "These utility functions handle boolean parsing, date conversion, \n",
    "foreign key lookups, and normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14f7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_bool(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "        return None\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    s = str(v).strip().lower()\n",
    "    return s in (\"true\", \"1\", \"t\", \"yes\", \"y\")\n",
    "\n",
    "def _clean_str(v, default=\"\"):\n",
    "    # Safely turn any value (including NaN/float) into a stripped string (or default)\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "        return default\n",
    "    s = str(v).strip()\n",
    "    return s if s else default\n",
    "\n",
    "def _resolve_user_id(conn, email=None, upn=None, external_id=None):\n",
    "    if email:\n",
    "        uid = conn.execute(text(\"SELECT user_id FROM users WHERE lower(email)=lower(:e)\"), {\"e\": email}).scalar()\n",
    "        if uid: return uid\n",
    "    if upn:\n",
    "        uid = conn.execute(text(\"SELECT user_id FROM users WHERE lower(upn)=lower(:u)\"), {\"u\": upn}).scalar()\n",
    "        if uid: return uid\n",
    "    if external_id:\n",
    "        uid = conn.execute(text(\"SELECT user_id FROM users WHERE external_user_id=:x\"), {\"x\": external_id}).scalar()\n",
    "        if uid: return uid\n",
    "    return None\n",
    "\n",
    "def _to_date(v, default=None):\n",
    "    # Accept strings like \"2024-07-01\", \"01/07/2024\", or excel-ish values\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or str(v).strip() == \"\":\n",
    "        return default\n",
    "    dt = pd.to_datetime(str(v).strip(), dayfirst=True, errors=\"coerce\")\n",
    "    return None if pd.isna(dt) else dt.date()\n",
    "        \n",
    "def _cv_id(conn, cv_partner_cv_id: str):\n",
    "    return conn.execute(\n",
    "        text(\"SELECT cv_id FROM cvs WHERE cv_partner_cv_id=:cid\"),\n",
    "        {\"cid\": str(cv_partner_cv_id)}\n",
    "    ).scalar()\n",
    "\n",
    "def _ensure_dim(conn, table: str, name: str, key: str = \"name\", id_col: str = None):\n",
    "    if not name:\n",
    "        return None\n",
    "    if id_col is None:\n",
    "        id_col = (table[4:] + \"_id\") if table.startswith(\"dim_\") else (table.rstrip(\"s\") + \"_id\")\n",
    "    conn.execute(text(f\"INSERT INTO {table} ({key}) VALUES (:n) ON CONFLICT ({key}) DO NOTHING\"), {\"n\": name})\n",
    "    return conn.execute(text(f\"SELECT {id_col} FROM {table} WHERE {key}=:n\"), {\"n\": name}).scalar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e33021",
   "metadata": {},
   "source": [
    "## Step 3.2 — Core entity upserts (Users and CVs)\n",
    "\n",
    "These upsert functions populate the `users` and `cvs` tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea8a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_users(conn, df):\n",
    "    print(f\"Upserting {len(df)} users.\")\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO users\n",
    "          (cv_partner_user_id, name_multilang, email, upn, external_user_id,\n",
    "           phone_number, landline, birth_year, department, country,\n",
    "           user_created_at, nationality_multilang)\n",
    "        VALUES\n",
    "          (:cv_partner_user_id, CAST(:name_multilang AS JSONB), :email, :upn, :external_user_id,\n",
    "           :phone_number, :landline, :birth_year, :department, :country,\n",
    "           :user_created_at, CAST(:nationality_multilang AS JSONB))\n",
    "        ON CONFLICT (cv_partner_user_id) DO UPDATE\n",
    "        SET name_multilang = EXCLUDED.name_multilang,\n",
    "            email = EXCLUDED.email,\n",
    "            upn = EXCLUDED.upn,\n",
    "            external_user_id = EXCLUDED.external_user_id,\n",
    "            phone_number = EXCLUDED.phone_number,\n",
    "            landline = EXCLUDED.landline,\n",
    "            birth_year = EXCLUDED.birth_year,\n",
    "            department = EXCLUDED.department,\n",
    "            country = EXCLUDED.country,\n",
    "            user_created_at = EXCLUDED.user_created_at,\n",
    "            nationality_multilang = EXCLUDED.nationality_multilang\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        conn.execute(sql, {\n",
    "            \"cv_partner_user_id\": str(r[\"CV Partner User ID\"]),\n",
    "            \"name_multilang\": json.dumps(r[\"Name (multilang)\"]),  # dict -> JSON\n",
    "            \"email\": r.get(\"Email\"),\n",
    "            \"upn\": r.get(\"UPN\"),\n",
    "            \"external_user_id\": r.get(\"External User ID\"),\n",
    "            \"phone_number\": r.get(\"Phone Number\"),\n",
    "            \"landline\": r.get(\"Landline\"),\n",
    "            \"birth_year\": int(r[\"Birth Year\"]) if pd.notna(r.get(\"Birth Year\")) else None,\n",
    "            \"department\": r.get(\"Department\"),\n",
    "            \"country\": r.get(\"Country\"),\n",
    "            \"user_created_at\": r.get(\"User created at\"),\n",
    "            \"nationality_multilang\": json.dumps(r.get(\"nationality_multilang\", {})),\n",
    "        })\n",
    "\n",
    "def upsert_cvs(conn, df):\n",
    "    print(f\"Upserting {len(df)} CVs...\")\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO cvs\n",
    "          (cv_partner_cv_id, user_id, title_multilang, years_of_education,\n",
    "           years_since_first_work_experience, has_profile_image,\n",
    "           owns_reference_project, read_privacy_notice,\n",
    "           cv_last_updated_by_owner, cv_last_updated,\n",
    "           sfia_level, cpd_level, cpd_band, cpd_label)\n",
    "        VALUES\n",
    "          (:cv_partner_cv_id, :user_id, CAST(:title_multilang AS JSONB), :yoe, :ysfwe,\n",
    "           :has_img, :owns_ref, :read_priv, :lu_owner, :lu,\n",
    "           :sfia_level, :cpd_level, :cpd_band, :cpd_label)\n",
    "        ON CONFLICT (cv_partner_cv_id) DO UPDATE\n",
    "        SET title_multilang = EXCLUDED.title_multilang,\n",
    "            years_of_education = EXCLUDED.years_of_education,\n",
    "            years_since_first_work_experience = EXCLUDED.years_since_first_work_experience,\n",
    "            has_profile_image = EXCLUDED.has_profile_image,\n",
    "            owns_reference_project = EXCLUDED.owns_reference_project,\n",
    "            read_privacy_notice = EXCLUDED.read_privacy_notice,\n",
    "            cv_last_updated_by_owner = EXCLUDED.cv_last_updated_by_owner,\n",
    "            cv_last_updated = EXCLUDED.cv_last_updated,\n",
    "            sfia_level = EXCLUDED.sfia_level,\n",
    "            cpd_level  = EXCLUDED.cpd_level,\n",
    "            cpd_band   = EXCLUDED.cpd_band,\n",
    "            cpd_label  = EXCLUDED.cpd_label\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        uid = conn.execute(\n",
    "            text(\"SELECT user_id FROM users WHERE cv_partner_user_id=:uid\"),\n",
    "            {\"uid\": str(r[\"CV Partner User ID\"])}\n",
    "        ).scalar()\n",
    "        if uid is None:\n",
    "            print(f\"  ⚠️ Skipping CV {r['CV Partner CV ID']} (unknown user {r['CV Partner User ID']})\")\n",
    "            continue\n",
    "\n",
    "        conn.execute(sql, {\n",
    "            \"cv_partner_cv_id\": str(r[\"CV Partner CV ID\"]),\n",
    "            \"user_id\": uid,\n",
    "            \"title_multilang\": json.dumps(r[\"title_multilang\"]),\n",
    "            \"yoe\": int(r[\"Years of education\"]) if pd.notna(r[\"Years of education\"]) else None,\n",
    "            \"ysfwe\": int(r[\"Years since first work experience\"]) if pd.notna(r[\"Years since first work experience\"]) else None,\n",
    "            \"has_img\": _to_bool(r[\"Has profile image\"]),\n",
    "            \"owns_ref\": _to_bool(r[\"Owns a reference project\"]),\n",
    "            \"read_priv\": _to_bool(r[\"Read and understood privacy notice\"]),\n",
    "            \"lu_owner\": r[\"CV Last updated by owner\"],\n",
    "            \"lu\": r[\"CV Last updated\"],\n",
    "            \"sfia_level\": r.get(\"sfia_level\"),\n",
    "            \"cpd_level\":  r.get(\"cpd_level\"),\n",
    "            \"cpd_band\":   (None if pd.isna(r.get(\"cpd_band\"))  else str(r.get(\"cpd_band\"))),\n",
    "            \"cpd_label\":  (None if pd.isna(r.get(\"cpd_label\")) else str(r.get(\"cpd_label\"))),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b25529",
   "metadata": {},
   "source": [
    "## Step 3.3 — Skills and languages\n",
    "\n",
    "Upserts for technology skills, languages, and related dimension tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67306ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_technologies(conn, df):\n",
    "    print(f\"Upserting {len(df)} technologies...\")\n",
    "    for _, r in df.iterrows():\n",
    "        tech_name = r[\"Skill name\"]\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO dim_technology (name)\n",
    "            VALUES (:name)\n",
    "            ON CONFLICT (name) DO NOTHING\n",
    "        \"\"\"), {\"name\": tech_name})\n",
    "\n",
    "        tech_id = conn.execute(\n",
    "            text(\"SELECT technology_id FROM dim_technology WHERE name=:n\"),\n",
    "            {\"n\": tech_name}\n",
    "        ).scalar()\n",
    "        if tech_id is None:\n",
    "            print(f\"  ⚠️ Skipping tech link; cannot resolve technology '{tech_name}'\")\n",
    "            continue\n",
    "\n",
    "        cv_id = conn.execute(\n",
    "            text(\"SELECT cv_id FROM cvs WHERE cv_partner_cv_id=:cid\"),\n",
    "            {\"cid\": str(r[\"CV Partner CV ID\"])}\n",
    "        ).scalar()\n",
    "        if cv_id is None:\n",
    "            print(f\"  ⚠️ Skipping tech link; unknown CV {r['CV Partner CV ID']}\")\n",
    "            continue\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO cv_technology (cv_id, technology_id, years_experience, proficiency, is_official_masterdata)\n",
    "            VALUES (:cv, :tech, :yexp, :prof, CAST(:is_md AS JSONB))\n",
    "            ON CONFLICT (cv_id, technology_id) DO UPDATE\n",
    "            SET years_experience = EXCLUDED.years_experience,\n",
    "                proficiency = EXCLUDED.proficiency,\n",
    "                is_official_masterdata = EXCLUDED.is_official_masterdata\n",
    "        \"\"\"), {\n",
    "            \"cv\": cv_id,\n",
    "            \"tech\": tech_id,\n",
    "            \"yexp\": int(r[\"Year experience\"]) if pd.notna(r[\"Year experience\"]) else None,\n",
    "            \"prof\": int(r[\"Proficiency (0-5)\"]) if pd.notna(r[\"Proficiency (0-5)\"]) else None,\n",
    "            \"is_md\": json.dumps(r[\"Is official masterdata (in #{lang})\"])  # dict -> json\n",
    "        })\n",
    "\n",
    "def upsert_languages(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} languages...\")\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO cv_language\n",
    "          (cv_id, language_id, level, highlighted, is_official_masterdata, updated, updated_by_owner)\n",
    "        VALUES\n",
    "          (:cv_id, :lang_id, :level, :highlighted, CAST(:is_md AS JSONB), :updated, :updated_by_owner)\n",
    "        ON CONFLICT (cv_id, language_id) DO UPDATE\n",
    "        SET level = EXCLUDED.level,\n",
    "            highlighted = EXCLUDED.highlighted,\n",
    "            is_official_masterdata = EXCLUDED.is_official_masterdata,\n",
    "            updated = EXCLUDED.updated,\n",
    "            updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        lang_id = _ensure_dim(conn, \"dim_language\", r.get(\"Language\"))\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"lang_id\": lang_id,\n",
    "            \"level\": r.get(\"Level\"),\n",
    "            \"highlighted\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"is_md\": json.dumps(r.get(\"Is official masterdata (in #{lang})\", {})),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff34cc",
   "metadata": {},
   "source": [
    "## Step 3.4 — Project experience, work experience, certifications, courses, education, and positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_project_experiences(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} project experiences...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO project_experience\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month_from, year_from, month_to, year_to,\n",
    "         customer_int, customer_multilang,\n",
    "         customer_anon_int, customer_anon_multilang,\n",
    "         description_int, description_multilang,\n",
    "         long_description_int, long_description_multilang,\n",
    "         industry_id, project_type_id,\n",
    "         percent_allocated, extent_individual_hours, extent_hours, extent_total_hours,\n",
    "         extent_unit, extent_currency, extent_total, extent_total_currency,\n",
    "         project_area, project_area_unit,\n",
    "         highlighted, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id,\n",
    "         :m_from, :y_from, :m_to, :y_to,\n",
    "         :cust_int, CAST(:cust_ml AS JSONB),\n",
    "         :cust_anon_int, CAST(:cust_anon_ml AS JSONB),\n",
    "         :desc_int, CAST(:desc_ml AS JSONB),\n",
    "         :ldesc_int, CAST(:ldesc_ml AS JSONB),\n",
    "         :industry_id, :project_type_id,\n",
    "         :pct_alloc, :indiv_hours, :hours, :total_hours,\n",
    "         :extent_unit, :extent_curr, :extent_total, :extent_total_curr,\n",
    "         :proj_area, :proj_area_unit,\n",
    "         :highlighted, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month_from = EXCLUDED.month_from, year_from = EXCLUDED.year_from,\n",
    "          month_to = EXCLUDED.month_to, year_to = EXCLUDED.year_to,\n",
    "          customer_int = EXCLUDED.customer_int, customer_multilang = EXCLUDED.customer_multilang,\n",
    "          customer_anon_int = EXCLUDED.customer_anon_int, customer_anon_multilang = EXCLUDED.customer_anon_multilang,\n",
    "          description_int = EXCLUDED.description_int, description_multilang = EXCLUDED.description_multilang,\n",
    "          long_description_int = EXCLUDED.long_description_int, long_description_multilang = EXCLUDED.long_description_multilang,\n",
    "          industry_id = EXCLUDED.industry_id, project_type_id = EXCLUDED.project_type_id,\n",
    "          percent_allocated = EXCLUDED.percent_allocated,\n",
    "          extent_individual_hours = EXCLUDED.extent_individual_hours,\n",
    "          extent_hours = EXCLUDED.extent_hours,\n",
    "          extent_total_hours = EXCLUDED.extent_total_hours,\n",
    "          extent_unit = EXCLUDED.extent_unit,\n",
    "          extent_currency = EXCLUDED.extent_currency,\n",
    "          extent_total = EXCLUDED.extent_total,\n",
    "          extent_total_currency = EXCLUDED.extent_total_currency,\n",
    "          project_area = EXCLUDED.project_area, project_area_unit = EXCLUDED.project_area_unit,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        industry_id = _ensure_dim(conn, \"dim_industry\", r.get(\"Industry (int)\"))\n",
    "        projtype_id = _ensure_dim(conn, \"dim_project_type\", r.get(\"Project type (int)\"))\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m_from\": r.get(\"Month from\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"m_to\": r.get(\"Month to\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"cust_int\": r.get(\"Customer (int)\"),\n",
    "            \"cust_ml\": json.dumps(r.get(\"Customer (#{lang})\", {})),\n",
    "            \"cust_anon_int\": r.get(\"Customer Anonymized (int)\"),\n",
    "            \"cust_anon_ml\": json.dumps(r.get(\"Customer Anonymized (#{lang})\", {})),\n",
    "            \"desc_int\": r.get(\"Description (int)\"),\n",
    "            \"desc_ml\": json.dumps(r.get(\"Description (#{lang})\", {})),\n",
    "            \"ldesc_int\": r.get(\"Long description (int)\"),\n",
    "            \"ldesc_ml\": json.dumps(r.get(\"Long description (#{lang})\", {})),\n",
    "            \"industry_id\": industry_id,\n",
    "            \"project_type_id\": projtype_id,\n",
    "            \"pct_alloc\": r.get(\"Percent allocated\"),\n",
    "            \"indiv_hours\": r.get(\"Project extent (individual hours)\"),\n",
    "            \"hours\": r.get(\"Project extent (hours)\"),\n",
    "            \"total_hours\": r.get(\"Project extent total (hours)\"),\n",
    "            \"extent_unit\": r.get(\"Project extent\"),\n",
    "            \"extent_curr\": r.get(\"Project extent (currency)\"),\n",
    "            \"extent_total\": r.get(\"Project extent total\"),\n",
    "            \"extent_total_curr\": r.get(\"Project extent total (currency)\"),\n",
    "            \"proj_area\": r.get(\"Project area\"),\n",
    "            \"proj_area_unit\": r.get(\"Project area (unit)\"),\n",
    "            \"highlighted\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_work_experiences(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} work experiences...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO work_experience\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month_from, year_from, month_to, year_to,\n",
    "         highlighted, employer, description, long_description,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id,\n",
    "         :m_from, :y_from, :m_to, :y_to,\n",
    "         :highlighted, :employer, :desc, :ldesc,\n",
    "         :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month_from = EXCLUDED.month_from, year_from = EXCLUDED.year_from,\n",
    "          month_to = EXCLUDED.month_to, year_to = EXCLUDED.year_to,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          employer = EXCLUDED.employer,\n",
    "          description = EXCLUDED.description,\n",
    "          long_description = EXCLUDED.long_description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m_from\": r.get(\"Month from\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"m_to\": r.get(\"Month to\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"highlighted\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"employer\": r.get(\"Employer\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"ldesc\": r.get(\"Long Description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_certifications(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} certifications...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO certification\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month, year, month_expire, year_expire,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :m, :y, :mexp, :yexp, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month = EXCLUDED.month, year = EXCLUDED.year,\n",
    "          month_expire = EXCLUDED.month_expire, year_expire = EXCLUDED.year_expire,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m\": r.get(\"Month\"),\n",
    "            \"y\": r.get(\"Year\"),\n",
    "            \"mexp\": r.get(\"Month expire\"),\n",
    "            \"yexp\": r.get(\"Year expire\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_courses(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} courses...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO course\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month, year, name, organiser, long_description, highlighted,\n",
    "         is_official_masterdata, attachments, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :m, :y, :name, :org, :ldesc, :hl,\n",
    "         CAST(:is_md AS JSONB), :att, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month = EXCLUDED.month, year = EXCLUDED.year,\n",
    "          name = EXCLUDED.name, organiser = EXCLUDED.organiser,\n",
    "          long_description = EXCLUDED.long_description,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          is_official_masterdata = EXCLUDED.is_official_masterdata,\n",
    "          attachments = EXCLUDED.attachments,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m\": r.get(\"Month\"),\n",
    "            \"y\": r.get(\"Year\"),\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"org\": r.get(\"Organiser\"),\n",
    "            \"ldesc\": r.get(\"Long description\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"is_md\": json.dumps(r.get(\"Is official masterdata (in #{lang})\", {})),\n",
    "            \"att\": r.get(\"Attachments\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_educations(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} educations...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO education\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month_from, year_from, month_to, year_to,\n",
    "         highlighted, attachments, place_of_study, degree, description,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id,\n",
    "         :m_from, :y_from, :m_to, :y_to,\n",
    "         :hl, :att, :place, :deg, :desc,\n",
    "         :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month_from = EXCLUDED.month_from, year_from = EXCLUDED.year_from,\n",
    "          month_to = EXCLUDED.month_to, year_to = EXCLUDED.year_to,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          attachments = EXCLUDED.attachments,\n",
    "          place_of_study = EXCLUDED.place_of_study,\n",
    "          degree = EXCLUDED.degree,\n",
    "          description = EXCLUDED.description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m_from\": r.get(\"Month from\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"m_to\": r.get(\"Month to\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"att\": r.get(\"Attachments\"),\n",
    "            \"place\": r.get(\"Place of study\"),\n",
    "            \"deg\": r.get(\"Degree\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_positions(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} positions...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO position\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         year_from, year_to, highlighted, name, description,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :y_from, :y_to, :hl, :name, :desc, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          year_from = EXCLUDED.year_from, year_to = EXCLUDED.year_to,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          name = EXCLUDED.name, description = EXCLUDED.description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_blogs(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} blogs/publications...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO blog_publication\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         name, description, highlighted, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :name, :desc, :hl, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          name = EXCLUDED.name, description = EXCLUDED.description,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_cv_roles(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} cv roles...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO cv_role\n",
    "        (cv_id, name, description, highlighted, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :name, :desc, :hl, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, name) DO UPDATE\n",
    "      SET description = EXCLUDED.description,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_key_qualifications(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} key qualifications...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO key_qualification\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         label, summary, short_description, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :label, :summary, :short_desc, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          label = EXCLUDED.label, summary = EXCLUDED.summary,\n",
    "          short_description = EXCLUDED.short_description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"label\": r.get(\"Label\"),\n",
    "            \"summary\": r.get(\"Summary of Qualifications\"),\n",
    "            \"short_desc\": r.get(\"Short description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac67bad",
   "metadata": {},
   "source": [
    "## Step 3.5 — Security clearance and availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d092d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_sc_clearance(conn, df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    for _, r in df.iterrows():\n",
    "        uid = _resolve_user_id(conn, r.get(\"Email\"), r.get(\"UPN\"), r.get(\"External User ID\"))\n",
    "        if not uid:\n",
    "            continue\n",
    "\n",
    "        clr = _clean_str(r.get(\"Clearance\"), \"None\") or \"None\"\n",
    "        conn.execute(text(\"INSERT INTO dim_clearance(name) VALUES (:n) ON CONFLICT(name) DO NOTHING\"),\n",
    "                     {\"n\": clr})\n",
    "        clr_id = conn.execute(text(\"SELECT clearance_id FROM dim_clearance WHERE name=:n\"),\n",
    "                              {\"n\": clr}).scalar()\n",
    "\n",
    "        # Default valid_from if missing so we never violate NOT NULL\n",
    "        vf = _to_date(r.get(\"Valid From\"), default=date(1900, 1, 1))\n",
    "        vt = _to_date(r.get(\"Valid To\"))\n",
    "        vb = _clean_str(r.get(\"Verified By\"), None) or None\n",
    "        no = _clean_str(r.get(\"Notes\"), None) or None\n",
    "\n",
    "        # If both present and vt < vf (bad data), drop vt\n",
    "        if vt and vf and vt < vf:\n",
    "            vt = None\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO user_clearance(user_id, clearance_id, valid_from, valid_to, verified_by, notes)\n",
    "            VALUES (:u, :c, :vf, :vt, :vb, :no)\n",
    "            ON CONFLICT (user_id, clearance_id, valid_from) DO UPDATE\n",
    "            SET valid_to    = EXCLUDED.valid_to,\n",
    "                verified_by = EXCLUDED.verified_by,\n",
    "                notes       = EXCLUDED.notes\n",
    "        \"\"\"), {\"u\": uid, \"c\": clr_id, \"vf\": vf, \"vt\": vt, \"vb\": vb, \"no\": no})\n",
    "\n",
    "\n",
    "\n",
    "def upsert_availability(conn, df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO user_availability(user_id, date, percent_available, source)\n",
    "        VALUES (:u, :d, :p, :s)\n",
    "        ON CONFLICT (user_id, date) DO UPDATE\n",
    "        SET percent_available = EXCLUDED.percent_available,\n",
    "            source            = EXCLUDED.source,\n",
    "            updated_at        = NOW()\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        uid = _resolve_user_id(conn, r.get(\"Email\"), r.get(\"UPN\"), r.get(\"External User ID\"))\n",
    "        if not uid:\n",
    "            continue\n",
    "        # percent can come as float/NaN — clamp to [0,100]\n",
    "        raw = r.get(\"Percent Available\")\n",
    "        p = 0 if (raw is None or (isinstance(raw, float) and pd.isna(raw))) else int(float(raw))\n",
    "        p = max(0, min(100, p))\n",
    "        conn.execute(sql, {\n",
    "            \"u\": uid,\n",
    "            \"d\": _clean_str(r.get(\"Date\"), None) or None,\n",
    "            \"p\": p,\n",
    "            \"s\": _clean_str(r.get(\"Source\"), \"Fake generator\"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d26263",
   "metadata": {},
   "source": [
    "## Step 3.6 — Load orchestrator\n",
    "\n",
    "The `load()` function runs all upserts in the correct sequence inside a \n",
    "single database transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b8135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple, clean load implementation below ---\n",
    "def load(clean_data, engine):\n",
    "    \"\"\"\n",
    "    Loads each cleaned DataFrame into the database using upsert logic.\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        if getattr(clean_data, 'users_df', None) is not None:\n",
    "            upsert_users(conn, clean_data.users_df)\n",
    "        if getattr(clean_data, 'cvs_df', None) is not None:\n",
    "            upsert_cvs(conn, clean_data.cvs_df)\n",
    "        if getattr(clean_data, 'technologies_df', None) is not None:\n",
    "            upsert_technologies(conn, clean_data.technologies_df)\n",
    "        if getattr(clean_data, 'languages_df', None) is not None:\n",
    "            upsert_languages(conn, clean_data.languages_df)\n",
    "        if getattr(clean_data, 'project_experiences_df', None) is not None:\n",
    "            upsert_project_experiences(conn, clean_data.project_experiences_df)\n",
    "        if getattr(clean_data, 'work_experiences_df', None) is not None:\n",
    "            upsert_work_experiences(conn, clean_data.work_experiences_df)\n",
    "        if getattr(clean_data, 'certifications_df', None) is not None:\n",
    "            upsert_certifications(conn, clean_data.certifications_df)\n",
    "        if getattr(clean_data, 'courses_df', None) is not None:\n",
    "            upsert_courses(conn, clean_data.courses_df)\n",
    "        if getattr(clean_data, 'educations_df', None) is not None:\n",
    "            upsert_educations(conn, clean_data.educations_df)\n",
    "        if getattr(clean_data, 'positions_df', None) is not None:\n",
    "            upsert_positions(conn, clean_data.positions_df)\n",
    "        if getattr(clean_data, 'blogs_df', None) is not None:\n",
    "            upsert_blogs(conn, clean_data.blogs_df)\n",
    "        if getattr(clean_data, 'cv_roles_df', None) is not None:\n",
    "            upsert_cv_roles(conn, clean_data.cv_roles_df)\n",
    "        if getattr(clean_data, 'key_qualifications_df', None) is not None:\n",
    "            upsert_key_qualifications(conn, clean_data.key_qualifications_df)\n",
    "        if getattr(clean_data, 'sc_clearance_df', None) is not None:\n",
    "            upsert_sc_clearance(conn, clean_data.sc_clearance_df)\n",
    "        if getattr(clean_data, 'availability_df', None) is not None:\n",
    "            upsert_availability(conn, clean_data.availability_df)\n",
    "    print(\"✅ Load complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025fc2e",
   "metadata": {},
   "source": [
    "# Step 4 — Database Setup\n",
    "\n",
    "These steps configure PostgreSQL connection settings, create the database if \n",
    "missing, and apply schema files from `sql/*.sql`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8be3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db_config_txt(path=\"db_config.txt\"):\n",
    "    db = {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if \"=\" in line:\n",
    "                    k, v = line.strip().split(\"=\", 1)\n",
    "                    db[k.strip()] = v.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return db\n",
    "\n",
    "def compose_settings():\n",
    "    # precedence: ENV > db_config.txt > defaults\n",
    "    defaults = dict(host=\"localhost\", port=5432, database=\"flowcase_demo\",\n",
    "                    user=\"postgres\", password=\"postgres\")\n",
    "    file_cfg = read_db_config_txt()\n",
    "\n",
    "    env_cfg = dict(\n",
    "        host=os.getenv(\"PGHOST\"),\n",
    "        port=os.getenv(\"PGPORT\"),\n",
    "        database=os.getenv(\"PGDATABASE\"),\n",
    "        user=os.getenv(\"PGUSER\"),\n",
    "        password=os.getenv(\"PGPASSWORD\"),\n",
    "    )\n",
    "    # drop Nones\n",
    "    env_cfg = {k:v for k,v in env_cfg.items() if v is not None}\n",
    "    # coerce port\n",
    "    if \"port\" in env_cfg:\n",
    "        try: env_cfg[\"port\"] = int(env_cfg[\"port\"])\n",
    "        except: env_cfg.pop(\"port\", None)\n",
    "\n",
    "    db = {**defaults, **file_cfg, **env_cfg}\n",
    "\n",
    "    settings = {\n",
    "        \"data_source\": \"fake\",\n",
    "        \"base_folder\": \"cv_reports\",\n",
    "        \"db\": db,\n",
    "        # let utils apply ALL sql/*.sql automatically\n",
    "        \"schema\": {\"apply_all_sql_in_sql_folder\": True, \"folder\": \"sql\"}\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "def ensure_database_exists(db):\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname=\"postgres\",\n",
    "                                user=db[\"user\"], password=db[\"password\"],\n",
    "                                host=db[\"host\"], port=db[\"port\"])\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (db[\"database\"],))\n",
    "        if not cur.fetchone():\n",
    "            print(f\"Database '{db['database']}' does not exist. Creating...\")\n",
    "            cur.execute(f\"CREATE DATABASE {db['database']};\")\n",
    "        cur.close(); conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not check/create database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0673d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_engine(db_settings: dict):\n",
    "    url = (\n",
    "        \"postgresql+psycopg2://\"\n",
    "        f\"{db_settings['user']}:{db_settings['password']}\"\n",
    "        f\"@{db_settings['host']}:{db_settings['port']}/{db_settings['database']}\"\n",
    "    )\n",
    "    return create_engine(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7c76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_database_schema_if_needed(engine, settings: dict):\n",
    "    schema_cfg = settings.get(\"schema\", {})\n",
    "    if not schema_cfg.get(\"apply_all_sql_in_sql_folder\"):\n",
    "        return\n",
    "\n",
    "    sql_folder = Path(schema_cfg.get(\"folder\", \"sql\"))\n",
    "    if not sql_folder.exists():\n",
    "        print(f\"Schema folder {sql_folder} does not exist. Skipping schema setup.\")\n",
    "        return\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        for path in sorted(sql_folder.glob(\"*.sql\")):\n",
    "            print(f\"Applying schema from {path.name}...\")\n",
    "            conn.execute(text(path.read_text()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a489cd8",
   "metadata": {},
   "source": [
    "# Step 5 — Run Full ETL Pipeline\n",
    "\n",
    "This executes:\n",
    "\n",
    "1. Extract  \n",
    "2. Transform  \n",
    "3. Load  \n",
    "\n",
    "And confirms everything worked end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91cda42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying schema from 01_schema.sql...\n",
      "\n",
      "==================== Finding the latest quarterly report folder ====================\n",
      "Quarterly folders found: ['Q42025']\n",
      "Using latest quarterly folder: Q42025\n",
      "\n",
      "==================== Loading CSV files from cv_reports/Q42025 ====================\n",
      "Found 15 CSV files.\n",
      "  Loaded certifications.csv -> (1006, 22)\n",
      "  Loaded project_experiences.csv -> (1501, 45)\n",
      "  Loaded blogs.csv -> (758, 21)\n",
      "  Loaded availability_report.csv -> (30000, 7)\n",
      "  Loaded cv_roles.csv -> (1001, 19)\n",
      "  Loaded work_experiences.csv -> (1484, 26)\n",
      "  Loaded educations.csv -> (757, 27)\n",
      "  Loaded user_report.csv -> (500, 26)\n",
      "  Loaded courses.csv -> (1446, 26)\n",
      "  Loaded key_qualifications.csv -> (485, 21)\n",
      "  Loaded positions.csv -> (1264, 23)\n",
      "  Loaded technologies.csv -> (2220, 20)\n",
      "  Loaded sc_clearance.csv -> (500, 9)\n",
      "  Loaded languages.csv -> (996, 22)\n",
      "  Loaded usage_report.csv -> (500, 51)\n",
      "Using data folder: cv_reports/Q42025\n",
      "Upserting 500 users.\n",
      "Upserting 500 CVs...\n",
      "Upserting 2220 technologies...\n",
      "Upserting 996 languages...\n",
      "Upserting 1501 project experiences...\n",
      "Upserting 1484 work experiences...\n",
      "Upserting 1006 certifications...\n",
      "Upserting 1446 courses...\n",
      "Upserting 757 educations...\n",
      "Upserting 1264 positions...\n",
      "Upserting 758 blogs/publications...\n",
      "Upserting 1001 cv roles...\n",
      "Upserting 485 key qualifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/rr2nrbcj6j79v6mwqtwtdzbw0000gn/T/ipykernel_26614/3417465587.py:32: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(str(v).strip(), dayfirst=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Load complete.\n",
      "✅ Flowcase ETL (manual fake) complete.\n"
     ]
    }
   ],
   "source": [
    "settings = compose_settings()\n",
    "db_settings = settings[\"db\"]\n",
    "\n",
    "ensure_database_exists(db_settings)\n",
    "engine = create_database_engine(db_settings)\n",
    "\n",
    "# 🔹 New line: apply schema from sql/*.sql\n",
    "setup_database_schema_if_needed(engine, settings)\n",
    "\n",
    "ex = extract(settings)\n",
    "print(f\"Using data folder: {getattr(ex, 'data_dir', 'unknown')}\")\n",
    "tr = transform(ex)\n",
    "\n",
    "load(tr, engine)\n",
    "print(\"✅ Flowcase ETL (manual fake) complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b25f8",
   "metadata": {},
   "source": [
    "## Step 5.1 — Basic database verification\n",
    "\n",
    "After running the full ETL, we check that key tables contain data as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac85034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in DB: 500\n",
      "CVs in DB: 500\n",
      "CV–technology links in DB: 2220\n",
      "✅ Basic load checks passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with engine.connect() as conn:\n",
    "    users_count = conn.execute(text(\"SELECT COUNT(*) FROM users\")).scalar()\n",
    "    cvs_count = conn.execute(text(\"SELECT COUNT(*) FROM cvs\")).scalar()\n",
    "    tech_links = conn.execute(text(\"SELECT COUNT(*) FROM cv_technology\")).scalar()\n",
    "\n",
    "print(f\"Users in DB: {users_count}\")\n",
    "print(f\"CVs in DB: {cvs_count}\")\n",
    "print(f\"CV–technology links in DB: {tech_links}\")\n",
    "\n",
    "assert users_count > 0, \"No users loaded!\"\n",
    "assert cvs_count > 0, \"No CVs loaded!\"\n",
    "print(\"✅ Basic load checks passed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
