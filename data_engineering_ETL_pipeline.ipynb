{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50201c6",
   "metadata": {},
   "source": [
    "# ETL Pipeline (Extract → Transform → Load)\n",
    "\n",
    "This notebook demonstrates the complete ETL pipeline for the Smart Assign MVP:\n",
    "\n",
    "1. Generate synthetic CV Partner–style reports  \n",
    "2. Extract raw CSVs  \n",
    "3. Transform them into a clean relational schema  \n",
    "4. Load into PostgreSQL  \n",
    "5. Perform basic verification  \n",
    "\n",
    "The goal is to demonstrate a clear, testable, step-by-step ETL workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13bbfdd",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. Create a Python virtual environment  \n",
    "2. Select the venv kernel  \n",
    "3. Install required dependencies (`pip install -r requirements.txt`)  \n",
    "4. Ensure PostgreSQL is running  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17b802",
   "metadata": {},
   "source": [
    "## Step 0 — Generate Synthetic Data\n",
    "\n",
    "We begin by generating synthetic Flowcase-style CV Partner reports.  \n",
    "These are stored under the `cv_reports/` folder using timestamped folder names.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d5506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ user_report.csv: 500 rows\n",
      "✔ usage_report.csv: 500 rows\n",
      "✔ project_experiences.csv: 1501 rows\n",
      "✔ certifications.csv: 1006 rows\n",
      "✔ courses.csv: 1446 rows\n",
      "✔ languages.csv: 996 rows\n",
      "✔ technologies.csv: 2220 rows\n",
      "✔ key_qualifications.csv: 485 rows\n",
      "✔ educations.csv: 757 rows\n",
      "✔ work_experiences.csv: 1484 rows\n",
      "✔ positions.csv: 1264 rows\n",
      "✔ blogs.csv: 758 rows\n",
      "✔ cv_roles.csv: 1001 rows\n",
      "✔ sc_clearance.csv: 500 rows\n",
      "✔ availability_report.csv: 30000 rows\n",
      "\n",
      "All files written under: /Users/alanabarrett-frew/Desktop/Module  4/Assignment/ETL Pipeline/preprocessing/ETL_pipeline/cv_reports/Q42025\n"
     ]
    }
   ],
   "source": [
    "import make_fake_flowcase_reports\n",
    "\n",
    "make_fake_flowcase_reports.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f28053",
   "metadata": {},
   "source": [
    "## Run all Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1373d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "from datetime import date\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "514123e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_step(title):\n",
    "    print(f\"\\n{'='*20} {title} {'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce0a69",
   "metadata": {},
   "source": [
    "# Step 1 — Extract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2036190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_report_folder(base_folder=\"cv_reports\"):\n",
    "    print_step(\"Finding the latest report folder\")\n",
    "\n",
    "    report_folders = [f for f in Path(base_folder).iterdir() if f.is_dir()]\n",
    "    if not report_folders:\n",
    "        raise FileNotFoundError(f\"No report folders found in {base_folder}\")\n",
    "\n",
    "    latest = sorted(report_folders, key=lambda f: f.name)[-1]\n",
    "\n",
    "    print(f\"Found {len(report_folders)} folders.\")\n",
    "    print(f\"Latest folder: {latest.name}\")\n",
    "\n",
    "    return latest\n",
    "\n",
    "\n",
    "def find_latest_quarterly_report_folder(base_folder=\"cv_reports\"):\n",
    "    print_step(\"Finding the latest quarterly report folder\")\n",
    "\n",
    "    pattern = re.compile(r\"Q[1-4]\\d{4}\")\n",
    "    report_folders = [\n",
    "        f for f in Path(base_folder).iterdir()\n",
    "        if f.is_dir() and pattern.match(f.name)\n",
    "    ]\n",
    "    if not report_folders:\n",
    "        raise FileNotFoundError(f\"No quarterly report folders found in {base_folder}.\")\n",
    "\n",
    "    names = sorted([f.name for f in report_folders])\n",
    "    print(\"Quarterly folders found:\", names)\n",
    "\n",
    "    latest_folder = sorted(report_folders, key=lambda folder: folder.name)[-1]\n",
    "    print(\"Using latest quarterly folder:\", latest_folder.name)\n",
    "\n",
    "    return latest_folder\n",
    "\n",
    "\n",
    "def load_csv_files_from_folder(report_folder):\n",
    "    print_step(f\"Loading CSV files from {report_folder}\")\n",
    "\n",
    "    csv_files = list(Path(report_folder).glob(\"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files.\")\n",
    "    dataframes = {}\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            dataframes[csv_file.name] = df\n",
    "            print(f\"  Loaded {csv_file.name} -> {df.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ⚠️ Failed to read {csv_file.name}: {e}\")\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def extract(settings):\n",
    "    \"\"\"\n",
    "    Finds the latest quarterly report folder and loads all CSVs as DataFrames.\n",
    "    Returns an object with .data_dir and dict-like data (keyed by filename).\n",
    "    \"\"\"\n",
    "    data_source = settings.get(\"data_source\", \"fake\")\n",
    "\n",
    "    if data_source == \"real\":\n",
    "        print(\"[extract] Real data mode selected, but not implemented.\")\n",
    "        return type(\"ExtractResult\", (), {\"data_dir\": None})()\n",
    "\n",
    "    # Fake data path\n",
    "    base_folder = settings.get(\"base_folder\", \"cv_reports\")\n",
    "    data_dir = find_latest_quarterly_report_folder(base_folder)\n",
    "    data = load_csv_files_from_folder(data_dir)\n",
    "\n",
    "    class ExtractResult(dict):\n",
    "        pass\n",
    "\n",
    "    result = ExtractResult(data)\n",
    "    result.data_dir = data_dir\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad32481",
   "metadata": {},
   "source": [
    "## Step 1.1 — Locate the latest report folder\n",
    "\n",
    "This identifies the most recent synthetic export under `cv_reports/`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "017eba07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 1.1: Locate latest report folder ====================\n",
      "\n",
      "==================== Finding the latest report folder ====================\n",
      "Found 1 folders.\n",
      "Latest folder: Q42025\n",
      "✅ Using folder: cv_reports/Q42025\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 1.1: Locate latest report folder\")\n",
    "\n",
    "latest_folder = find_latest_report_folder(\"cv_reports\")\n",
    "print(f\"✅ Using folder: {latest_folder}\")\n",
    "\n",
    "# Tiny test: does it actually exist?\n",
    "assert latest_folder.exists(), \"Latest folder path does not exist on disk!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a88e7",
   "metadata": {},
   "source": [
    "## Step 1.2 — Load all CSV files from the latest report\n",
    "\n",
    "We load all CSVs inside the selected folder into pandas DataFrames and perform a\n",
    "small sanity check to ensure core files are present.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30eb83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 1.2: Load CSV files from latest folder ====================\n",
      "\n",
      "==================== Loading CSV files from cv_reports/Q42025 ====================\n",
      "Found 15 CSV files.\n",
      "  Loaded certifications.csv -> (1006, 22)\n",
      "  Loaded project_experiences.csv -> (1501, 45)\n",
      "  Loaded blogs.csv -> (758, 21)\n",
      "  Loaded availability_report.csv -> (30000, 7)\n",
      "  Loaded cv_roles.csv -> (1001, 19)\n",
      "  Loaded work_experiences.csv -> (1484, 26)\n",
      "  Loaded educations.csv -> (757, 27)\n",
      "  Loaded user_report.csv -> (500, 26)\n",
      "  Loaded courses.csv -> (1446, 26)\n",
      "  Loaded key_qualifications.csv -> (485, 21)\n",
      "  Loaded positions.csv -> (1264, 23)\n",
      "  Loaded technologies.csv -> (2220, 20)\n",
      "  Loaded sc_clearance.csv -> (500, 9)\n",
      "  Loaded languages.csv -> (996, 22)\n",
      "  Loaded usage_report.csv -> (500, 51)\n",
      "\n",
      "Summary of loaded files:\n",
      " - certifications.csv             (1006, 22)\n",
      " - project_experiences.csv        (1501, 45)\n",
      " - blogs.csv                      (758, 21)\n",
      " - availability_report.csv        (30000, 7)\n",
      " - cv_roles.csv                   (1001, 19)\n",
      " - work_experiences.csv           (1484, 26)\n",
      " - educations.csv                 (757, 27)\n",
      " - user_report.csv                (500, 26)\n",
      " - courses.csv                    (1446, 26)\n",
      " - key_qualifications.csv         (485, 21)\n",
      " - positions.csv                  (1264, 23)\n",
      " - technologies.csv               (2220, 20)\n",
      " - sc_clearance.csv               (500, 9)\n",
      " - languages.csv                  (996, 22)\n",
      " - usage_report.csv               (500, 51)\n",
      "\n",
      "Expected core files: ['user_report.csv', 'project_experiences.csv', 'work_experiences.csv']\n",
      "Missing: []\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 1.2: Load CSV files from latest folder\")\n",
    "\n",
    "raw_frames = load_csv_files_from_folder(latest_folder)\n",
    "\n",
    "print(\"\\nSummary of loaded files:\")\n",
    "for name, df in raw_frames.items():\n",
    "    print(f\" - {name:30} {df.shape}\")\n",
    "\n",
    "# Simple check: do we have some expected core CSVs?\n",
    "expected_files = [\n",
    "    \"user_report.csv\",\n",
    "    \"project_experiences.csv\",\n",
    "    \"work_experiences.csv\",\n",
    "]\n",
    "missing = [f for f in expected_files if f not in raw_frames]\n",
    "\n",
    "print(\"\\nExpected core files:\", expected_files)\n",
    "print(\"Missing:\", missing)\n",
    "\n",
    "assert not missing, \"One or more expected CSVs are missing!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bae474",
   "metadata": {},
   "source": [
    "# Step 2 — Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93e221",
   "metadata": {},
   "source": [
    "## Step 2.1 — Transform helpers\n",
    "\n",
    "These helpers normalise multilang fields, dates, and define the `TransformResult`\n",
    "dataclass for returning a clean set of tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d4ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformResult:\n",
    "    users_df: pd.DataFrame | None = None\n",
    "    cvs_df: pd.DataFrame | None = None\n",
    "    technologies_df: pd.DataFrame | None = None\n",
    "    languages_df: pd.DataFrame | None = None\n",
    "    project_experiences_df: pd.DataFrame | None = None\n",
    "    work_experiences_df: pd.DataFrame | None = None\n",
    "    certifications_df: pd.DataFrame | None = None\n",
    "    courses_df: pd.DataFrame | None = None\n",
    "    educations_df: pd.DataFrame | None = None\n",
    "    positions_df: pd.DataFrame | None = None\n",
    "    blogs_df: pd.DataFrame | None = None\n",
    "    cv_roles_df: pd.DataFrame | None = None\n",
    "    key_qualifications_df: pd.DataFrame | None = None\n",
    "    sc_clearance_df: pd.DataFrame | None = None\n",
    "    availability_df: pd.DataFrame | None = None\n",
    "\n",
    "def parse_multilang(pipe: object) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a single pipe string like 'int:Text|no:Tekst' into a dict.\n",
    "    Anything non-string or blank -> {}.\n",
    "    \"\"\"\n",
    "    if not isinstance(pipe, str) or not pipe.strip():\n",
    "        return {}\n",
    "    out = {}\n",
    "    for part in pipe.split(\"|\"):\n",
    "        if \":\" in part:\n",
    "            k, v = part.split(\":\", 1)\n",
    "            k, v = k.strip(), v.strip()\n",
    "            if k and v:\n",
    "                out[k] = v\n",
    "    return out\n",
    "\n",
    "def to_iso_date(s: object) -> str | None:\n",
    "    if s is None or (isinstance(s, float) and pd.isna(s)):\n",
    "        return None\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # If it looks like ISO (yyyy-mm-dd), parse straight without dayfirst\n",
    "    if \"-\" in s and len(s.split(\"-\")[0]) == 4:\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "    else:\n",
    "        dt = pd.to_datetime(s, dayfirst=True, errors=\"coerce\")\n",
    "    return None if pd.isna(dt) else dt.date().isoformat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba73a74",
   "metadata": {},
   "source": [
    "## Step 2.2 — Core transform logic\n",
    "\n",
    "The `transform()` function builds clean tables for Users, CVs, skills, \n",
    "experiences, and other CV Partner sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f5fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data) -> TransformResult:\n",
    "    # Core extracts\n",
    "    users = data.get(\"user_report.csv\", pd.DataFrame()).copy()\n",
    "    usage = data.get(\"usage_report.csv\", pd.DataFrame()).copy()\n",
    "\n",
    "    # Parse user name as dict\n",
    "    if not users.empty and \"Name (multilang)\" in users.columns:\n",
    "        users[\"Name (multilang)\"] = users[\"Name (multilang)\"].map(parse_multilang)\n",
    "    else:\n",
    "        users[\"Name (multilang)\"] = [{}] * len(users)\n",
    "\n",
    "    # nationality comes from usage_report: \"Nationality (#{lang})\" is a single pipe string\n",
    "    if not usage.empty and \"Nationality (#{lang})\" in usage.columns:\n",
    "        nat_map = {\n",
    "            str(r[\"CV Partner User ID\"]): parse_multilang(r[\"Nationality (#{lang})\"])\n",
    "            for _, r in usage.iterrows()\n",
    "            if \"CV Partner User ID\" in r and pd.notna(r[\"CV Partner User ID\"])\n",
    "        }\n",
    "        users[\"nationality_multilang\"] = users[\"CV Partner User ID\"].map(\n",
    "            lambda uid: nat_map.get(str(uid), {})\n",
    "        )\n",
    "    else:\n",
    "        users[\"nationality_multilang\"] = [{}] * len(users)\n",
    "\n",
    "    # Build CV rows: your user_report already has one row per CV\n",
    "    cvs = users.copy()\n",
    "    if \"Title (#{lang})\" in users.columns:\n",
    "        cvs[\"title_multilang\"] = users[\"Title (#{lang})\"].map(parse_multilang)\n",
    "    else:\n",
    "        cvs[\"title_multilang\"] = [{}] * len(cvs)\n",
    "\n",
    "    # Carry seniority columns from user_report -> cvs_df\n",
    "    def _num(x):\n",
    "        try:\n",
    "            return int(x)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    cvs[\"sfia_level\"] = users.get(\"SFIA Level\", pd.Series([None]*len(users))).map(_num)\n",
    "    cvs[\"cpd_level\"]  = users.get(\"CPD Level\",  pd.Series([None]*len(users))).map(_num)\n",
    "    cvs[\"cpd_band\"]   = users.get(\"CPD Band\",   pd.Series([None]*len(users))).astype(\"string\").where(lambda s: s.notna(), None)\n",
    "    cvs[\"cpd_label\"]  = users.get(\"CPD Label\",  pd.Series([None]*len(users))).astype(\"string\").where(lambda s: s.notna(), None)\n",
    "\n",
    "\n",
    "    # Optional extras (pass through + light cleanup)\n",
    "    sc_clearance = data.get(\"sc_clearance.csv\", pd.DataFrame()).copy()\n",
    "    if not sc_clearance.empty:\n",
    "        for col in (\"Valid From\", \"Valid To\"):\n",
    "            if col in sc_clearance.columns:\n",
    "                sc_clearance[col] = sc_clearance[col].map(to_iso_date)\n",
    "\n",
    "    availability = data.get(\"availability_report.csv\", pd.DataFrame()).copy()\n",
    "    if not availability.empty and \"Date\" in availability.columns:\n",
    "        availability[\"Date\"] = availability[\"Date\"].map(to_iso_date)\n",
    "\n",
    "    return TransformResult(\n",
    "        users_df=users if not users.empty else pd.DataFrame(),\n",
    "        cvs_df=cvs if not cvs.empty else pd.DataFrame(),\n",
    "        technologies_df=data.get(\"technologies.csv\"),\n",
    "        languages_df=data.get(\"languages.csv\"),\n",
    "        project_experiences_df=data.get(\"project_experiences.csv\"),\n",
    "        work_experiences_df=data.get(\"work_experiences.csv\"),\n",
    "        certifications_df=data.get(\"certifications.csv\"),\n",
    "        courses_df=data.get(\"courses.csv\"),\n",
    "        educations_df=data.get(\"educations.csv\"),\n",
    "        positions_df=data.get(\"positions.csv\"),\n",
    "        blogs_df=data.get(\"blogs.csv\"),\n",
    "        cv_roles_df=data.get(\"cv_roles.csv\"),\n",
    "        key_qualifications_df=data.get(\"key_qualifications.csv\"),\n",
    "        sc_clearance_df=sc_clearance if not sc_clearance.empty else pd.DataFrame(),\n",
    "        availability_df=availability if not availability.empty else pd.DataFrame(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55720d32",
   "metadata": {},
   "source": [
    "## Step 2.3 — Run transform and perform tests\n",
    "\n",
    "We now execute `transform(raw_data)` and validate:\n",
    "\n",
    "- Row counts  \n",
    "- Key identifiers  \n",
    "- Data integrity (e.g., CV count aligns with user count)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297eed50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 2.3: Reload CSVs for transform demo ====================\n",
      "\n",
      "==================== Loading CSV files from cv_reports/Q42025 ====================\n",
      "Found 15 CSV files.\n",
      "  Loaded certifications.csv -> (1006, 22)\n",
      "  Loaded project_experiences.csv -> (1501, 45)\n",
      "  Loaded blogs.csv -> (758, 21)\n",
      "  Loaded availability_report.csv -> (30000, 7)\n",
      "  Loaded cv_roles.csv -> (1001, 19)\n",
      "  Loaded work_experiences.csv -> (1484, 26)\n",
      "  Loaded educations.csv -> (757, 27)\n",
      "  Loaded user_report.csv -> (500, 26)\n",
      "  Loaded courses.csv -> (1446, 26)\n",
      "  Loaded key_qualifications.csv -> (485, 21)\n",
      "  Loaded positions.csv -> (1264, 23)\n",
      "  Loaded technologies.csv -> (2220, 20)\n",
      "  Loaded sc_clearance.csv -> (500, 9)\n",
      "  Loaded languages.csv -> (996, 22)\n",
      "  Loaded usage_report.csv -> (500, 51)\n",
      "\n",
      "==================== Step 2.3a: Run transform() on extracted data ====================\n",
      "\n",
      "Transformed tables (row counts):\n",
      " - users_df                    500 rows\n",
      " - cvs_df                      500 rows\n",
      " - technologies_df            2220 rows\n",
      " - languages_df                996 rows\n",
      " - project_experiences_df     1501 rows\n",
      " - work_experiences_df        1484 rows\n",
      "\n",
      "Sample: users_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Name (multilang)</th>\n",
       "      <th>Title (#{lang})</th>\n",
       "      <th>Email</th>\n",
       "      <th>UPN</th>\n",
       "      <th>External User ID</th>\n",
       "      <th>CV Partner User ID</th>\n",
       "      <th>CV Partner CV ID</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Landline</th>\n",
       "      <th>...</th>\n",
       "      <th>Years since first work experience</th>\n",
       "      <th>Access roles</th>\n",
       "      <th>Has profile image</th>\n",
       "      <th>Owns a reference project</th>\n",
       "      <th>Read and understood privacy notice</th>\n",
       "      <th>SFIA Level</th>\n",
       "      <th>CPD Level</th>\n",
       "      <th>CPD Band</th>\n",
       "      <th>CPD Label</th>\n",
       "      <th>nationality_multilang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Johnson</td>\n",
       "      <td>{'int': 'Danielle Johnson'}</td>\n",
       "      <td>int:Principal C# Developer</td>\n",
       "      <td>danielle.johnson@mail.test</td>\n",
       "      <td>daniellejohnson</td>\n",
       "      <td>ext_b89ca31a</td>\n",
       "      <td>b89ca31a</td>\n",
       "      <td>cv_b89ca31a</td>\n",
       "      <td>958-350-6431</td>\n",
       "      <td>+1-539-500-5329x31839</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>User</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Norwegian'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joshua Walker</td>\n",
       "      <td>{'int': 'Joshua Walker'}</td>\n",
       "      <td>int:Principal Data Engineer</td>\n",
       "      <td>joshua.walker@mail.test</td>\n",
       "      <td>joshuawalker</td>\n",
       "      <td>ext_74b836a3</td>\n",
       "      <td>74b836a3</td>\n",
       "      <td>cv_74b836a3</td>\n",
       "      <td>729-504-2284x21020</td>\n",
       "      <td>001-350-324-0268</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>User</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Swedish'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jill Rhodes</td>\n",
       "      <td>{'int': 'Jill Rhodes'}</td>\n",
       "      <td>int:Senior C# Developer</td>\n",
       "      <td>jill.rhodes@example.org</td>\n",
       "      <td>jillrhodes</td>\n",
       "      <td>ext_70aaa8ab</td>\n",
       "      <td>70aaa8ab</td>\n",
       "      <td>cv_70aaa8ab</td>\n",
       "      <td>975.289.1783x9084</td>\n",
       "      <td>7764617711</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>User</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>{'int': 'British'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patricia Miller</td>\n",
       "      <td>{'int': 'Patricia Miller'}</td>\n",
       "      <td>int:Principal Analytics Engineer</td>\n",
       "      <td>patricia.miller@mail.test</td>\n",
       "      <td>patriciamiller</td>\n",
       "      <td>ext_080ef439</td>\n",
       "      <td>080ef439</td>\n",
       "      <td>cv_080ef439</td>\n",
       "      <td>624-999-8569</td>\n",
       "      <td>896.311.8367x36576</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>User</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Polish'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>{'int': 'Robert Johnson'}</td>\n",
       "      <td>int:Associate ML Engineer</td>\n",
       "      <td>robert.johnson@mail.test</td>\n",
       "      <td>robertjohnson</td>\n",
       "      <td>ext_4253cf51</td>\n",
       "      <td>4253cf51</td>\n",
       "      <td>cv_4253cf51</td>\n",
       "      <td>465-245-2711x11615</td>\n",
       "      <td>001-688-651-6560</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>Country Manager</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>{'int': 'Danish'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name             Name (multilang)  \\\n",
       "0  Danielle Johnson  {'int': 'Danielle Johnson'}   \n",
       "1     Joshua Walker     {'int': 'Joshua Walker'}   \n",
       "2       Jill Rhodes       {'int': 'Jill Rhodes'}   \n",
       "3   Patricia Miller   {'int': 'Patricia Miller'}   \n",
       "4    Robert Johnson    {'int': 'Robert Johnson'}   \n",
       "\n",
       "                    Title (#{lang})                       Email  \\\n",
       "0        int:Principal C# Developer  danielle.johnson@mail.test   \n",
       "1       int:Principal Data Engineer     joshua.walker@mail.test   \n",
       "2           int:Senior C# Developer     jill.rhodes@example.org   \n",
       "3  int:Principal Analytics Engineer   patricia.miller@mail.test   \n",
       "4         int:Associate ML Engineer    robert.johnson@mail.test   \n",
       "\n",
       "               UPN External User ID CV Partner User ID CV Partner CV ID  \\\n",
       "0  daniellejohnson     ext_b89ca31a           b89ca31a      cv_b89ca31a   \n",
       "1     joshuawalker     ext_74b836a3           74b836a3      cv_74b836a3   \n",
       "2       jillrhodes     ext_70aaa8ab           70aaa8ab      cv_70aaa8ab   \n",
       "3   patriciamiller     ext_080ef439           080ef439      cv_080ef439   \n",
       "4    robertjohnson     ext_4253cf51           4253cf51      cv_4253cf51   \n",
       "\n",
       "         Phone Number               Landline  ...  \\\n",
       "0        958-350-6431  +1-539-500-5329x31839  ...   \n",
       "1  729-504-2284x21020       001-350-324-0268  ...   \n",
       "2   975.289.1783x9084             7764617711  ...   \n",
       "3        624-999-8569     896.311.8367x36576  ...   \n",
       "4  465-245-2711x11615       001-688-651-6560  ...   \n",
       "\n",
       "   Years since first work experience     Access roles Has profile image  \\\n",
       "0                                  5             User              True   \n",
       "1                                 12             User             False   \n",
       "2                                  9             User              True   \n",
       "3                                 18             User             False   \n",
       "4                                 10  Country Manager              True   \n",
       "\n",
       "  Owns a reference project Read and understood privacy notice SFIA Level  \\\n",
       "0                    False                               True          5   \n",
       "1                    False                              False          5   \n",
       "2                    False                               True          4   \n",
       "3                    False                               True          5   \n",
       "4                     True                              False          2   \n",
       "\n",
       "   CPD Level  CPD Band CPD Label  nationality_multilang  \n",
       "0          3         L     CPD3L   {'int': 'Norwegian'}  \n",
       "1          3         L     CPD3L     {'int': 'Swedish'}  \n",
       "2          3         E     CPD3E     {'int': 'British'}  \n",
       "3          3         L     CPD3L      {'int': 'Polish'}  \n",
       "4          1         E     CPD1E      {'int': 'Danish'}  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample: cvs_df\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Name (multilang)</th>\n",
       "      <th>Title (#{lang})</th>\n",
       "      <th>Email</th>\n",
       "      <th>UPN</th>\n",
       "      <th>External User ID</th>\n",
       "      <th>CV Partner User ID</th>\n",
       "      <th>CV Partner CV ID</th>\n",
       "      <th>Phone Number</th>\n",
       "      <th>Landline</th>\n",
       "      <th>...</th>\n",
       "      <th>SFIA Level</th>\n",
       "      <th>CPD Level</th>\n",
       "      <th>CPD Band</th>\n",
       "      <th>CPD Label</th>\n",
       "      <th>nationality_multilang</th>\n",
       "      <th>title_multilang</th>\n",
       "      <th>sfia_level</th>\n",
       "      <th>cpd_level</th>\n",
       "      <th>cpd_band</th>\n",
       "      <th>cpd_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Danielle Johnson</td>\n",
       "      <td>{'int': 'Danielle Johnson'}</td>\n",
       "      <td>int:Principal C# Developer</td>\n",
       "      <td>danielle.johnson@mail.test</td>\n",
       "      <td>daniellejohnson</td>\n",
       "      <td>ext_b89ca31a</td>\n",
       "      <td>b89ca31a</td>\n",
       "      <td>cv_b89ca31a</td>\n",
       "      <td>958-350-6431</td>\n",
       "      <td>+1-539-500-5329x31839</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Norwegian'}</td>\n",
       "      <td>{'int': 'Principal C# Developer'}</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joshua Walker</td>\n",
       "      <td>{'int': 'Joshua Walker'}</td>\n",
       "      <td>int:Principal Data Engineer</td>\n",
       "      <td>joshua.walker@mail.test</td>\n",
       "      <td>joshuawalker</td>\n",
       "      <td>ext_74b836a3</td>\n",
       "      <td>74b836a3</td>\n",
       "      <td>cv_74b836a3</td>\n",
       "      <td>729-504-2284x21020</td>\n",
       "      <td>001-350-324-0268</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Swedish'}</td>\n",
       "      <td>{'int': 'Principal Data Engineer'}</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jill Rhodes</td>\n",
       "      <td>{'int': 'Jill Rhodes'}</td>\n",
       "      <td>int:Senior C# Developer</td>\n",
       "      <td>jill.rhodes@example.org</td>\n",
       "      <td>jillrhodes</td>\n",
       "      <td>ext_70aaa8ab</td>\n",
       "      <td>70aaa8ab</td>\n",
       "      <td>cv_70aaa8ab</td>\n",
       "      <td>975.289.1783x9084</td>\n",
       "      <td>7764617711</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>{'int': 'British'}</td>\n",
       "      <td>{'int': 'Senior C# Developer'}</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD3E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Patricia Miller</td>\n",
       "      <td>{'int': 'Patricia Miller'}</td>\n",
       "      <td>int:Principal Analytics Engineer</td>\n",
       "      <td>patricia.miller@mail.test</td>\n",
       "      <td>patriciamiller</td>\n",
       "      <td>ext_080ef439</td>\n",
       "      <td>080ef439</td>\n",
       "      <td>cv_080ef439</td>\n",
       "      <td>624-999-8569</td>\n",
       "      <td>896.311.8367x36576</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>{'int': 'Polish'}</td>\n",
       "      <td>{'int': 'Principal Analytics Engineer'}</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD3L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>{'int': 'Robert Johnson'}</td>\n",
       "      <td>int:Associate ML Engineer</td>\n",
       "      <td>robert.johnson@mail.test</td>\n",
       "      <td>robertjohnson</td>\n",
       "      <td>ext_4253cf51</td>\n",
       "      <td>4253cf51</td>\n",
       "      <td>cv_4253cf51</td>\n",
       "      <td>465-245-2711x11615</td>\n",
       "      <td>001-688-651-6560</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>{'int': 'Danish'}</td>\n",
       "      <td>{'int': 'Associate ML Engineer'}</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>CPD1E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name             Name (multilang)  \\\n",
       "0  Danielle Johnson  {'int': 'Danielle Johnson'}   \n",
       "1     Joshua Walker     {'int': 'Joshua Walker'}   \n",
       "2       Jill Rhodes       {'int': 'Jill Rhodes'}   \n",
       "3   Patricia Miller   {'int': 'Patricia Miller'}   \n",
       "4    Robert Johnson    {'int': 'Robert Johnson'}   \n",
       "\n",
       "                    Title (#{lang})                       Email  \\\n",
       "0        int:Principal C# Developer  danielle.johnson@mail.test   \n",
       "1       int:Principal Data Engineer     joshua.walker@mail.test   \n",
       "2           int:Senior C# Developer     jill.rhodes@example.org   \n",
       "3  int:Principal Analytics Engineer   patricia.miller@mail.test   \n",
       "4         int:Associate ML Engineer    robert.johnson@mail.test   \n",
       "\n",
       "               UPN External User ID CV Partner User ID CV Partner CV ID  \\\n",
       "0  daniellejohnson     ext_b89ca31a           b89ca31a      cv_b89ca31a   \n",
       "1     joshuawalker     ext_74b836a3           74b836a3      cv_74b836a3   \n",
       "2       jillrhodes     ext_70aaa8ab           70aaa8ab      cv_70aaa8ab   \n",
       "3   patriciamiller     ext_080ef439           080ef439      cv_080ef439   \n",
       "4    robertjohnson     ext_4253cf51           4253cf51      cv_4253cf51   \n",
       "\n",
       "         Phone Number               Landline  ...  SFIA Level CPD Level  \\\n",
       "0        958-350-6431  +1-539-500-5329x31839  ...           5         3   \n",
       "1  729-504-2284x21020       001-350-324-0268  ...           5         3   \n",
       "2   975.289.1783x9084             7764617711  ...           4         3   \n",
       "3        624-999-8569     896.311.8367x36576  ...           5         3   \n",
       "4  465-245-2711x11615       001-688-651-6560  ...           2         1   \n",
       "\n",
       "  CPD Band CPD Label nationality_multilang  \\\n",
       "0        L     CPD3L  {'int': 'Norwegian'}   \n",
       "1        L     CPD3L    {'int': 'Swedish'}   \n",
       "2        E     CPD3E    {'int': 'British'}   \n",
       "3        L     CPD3L     {'int': 'Polish'}   \n",
       "4        E     CPD1E     {'int': 'Danish'}   \n",
       "\n",
       "                           title_multilang  sfia_level  cpd_level cpd_band  \\\n",
       "0        {'int': 'Principal C# Developer'}           5          3        L   \n",
       "1       {'int': 'Principal Data Engineer'}           5          3        L   \n",
       "2           {'int': 'Senior C# Developer'}           4          3        E   \n",
       "3  {'int': 'Principal Analytics Engineer'}           5          3        L   \n",
       "4         {'int': 'Associate ML Engineer'}           2          1        E   \n",
       "\n",
       "   cpd_label  \n",
       "0      CPD3L  \n",
       "1      CPD3L  \n",
       "2      CPD3E  \n",
       "3      CPD3L  \n",
       "4      CPD1E  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 2.3b: Basic data quality checks on transform output ====================\n",
      "✅ Transform checks passed for users_df and cvs_df.\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 2.3: Reload CSVs for transform demo\")\n",
    "\n",
    "raw_data = load_csv_files_from_folder(latest_folder)\n",
    "\n",
    "print_step(\"Step 2.3a: Run transform() on extracted data\")\n",
    "\n",
    "# Use the raw data dict from load_csv_files_from_folder(...)\n",
    "tr = transform(raw_data)\n",
    "\n",
    "# Basic shape summary\n",
    "print(\"\\nTransformed tables (row counts):\")\n",
    "for name in [\n",
    "    \"users_df\", \"cvs_df\", \"technologies_df\", \"languages_df\",\n",
    "    \"project_experiences_df\", \"work_experiences_df\",\n",
    "]:\n",
    "    df = getattr(tr, name)\n",
    "    if df is not None:\n",
    "        print(f\" - {name:25} {len(df):5d} rows\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nSample: users_df\")\n",
    "display(tr.users_df.head())\n",
    "\n",
    "print(\"\\nSample: cvs_df\")\n",
    "display(tr.cvs_df.head())\n",
    "\n",
    "print_step(\"Step 2.3b: Basic data quality checks on transform output\")\n",
    "\n",
    "users_df = tr.users_df\n",
    "cvs_df = tr.cvs_df\n",
    "\n",
    "# 1) Ensure we actually have users\n",
    "assert not users_df.empty, \"users_df is unexpectedly empty after transform!\"\n",
    "\n",
    "# 2) Key identifier should exist & not be all null\n",
    "assert \"CV Partner User ID\" in users_df.columns, \"Missing CV Partner User ID column in users_df\"\n",
    "assert users_df[\"CV Partner User ID\"].notna().any(), \"All user IDs are null!\"\n",
    "\n",
    "# 3) Same number of rows in users_df and cvs_df (since user_report is 1 CV per row)\n",
    "assert len(users_df) == len(cvs_df), \"users_df and cvs_df row counts differ!\"\n",
    "\n",
    "print(\"✅ Transform checks passed for users_df and cvs_df.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377b6c6",
   "metadata": {},
   "source": [
    "# Step 3 — Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae8579",
   "metadata": {},
   "source": [
    "## Step 3.1 — Load helpers\n",
    "\n",
    "These utility functions handle boolean parsing, date conversion, \n",
    "foreign key lookups, and normalisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14f7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_bool(v):\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "        return None\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    s = str(v).strip().lower()\n",
    "    return s in (\"true\", \"1\", \"t\", \"yes\", \"y\")\n",
    "\n",
    "def _clean_str(v, default=\"\"):\n",
    "    # Safely turn any value (including NaN/float) into a stripped string (or default)\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "        return default\n",
    "    s = str(v).strip()\n",
    "    return s if s else default\n",
    "\n",
    "def _resolve_user_id(conn, email=None, upn=None, external_id=None):\n",
    "    if email:\n",
    "        uid = conn.execute(text(\"SELECT user_id FROM users WHERE lower(email)=lower(:e)\"), {\"e\": email}).scalar()\n",
    "        if uid: return uid\n",
    "    if upn:\n",
    "        uid = conn.execute(text(\"SELECT user_id FROM users WHERE lower(upn)=lower(:u)\"), {\"u\": upn}).scalar()\n",
    "        if uid: return uid\n",
    "    if external_id:\n",
    "        uid = conn.execute(text(\"SELECT user_id FROM users WHERE external_user_id=:x\"), {\"x\": external_id}).scalar()\n",
    "        if uid: return uid\n",
    "    return None\n",
    "\n",
    "def _to_date(v, default=None):\n",
    "    # Accept strings like \"2024-07-01\", \"01/07/2024\", or excel-ish values\n",
    "    if v is None or (isinstance(v, float) and pd.isna(v)) or str(v).strip() == \"\":\n",
    "        return default\n",
    "    dt = pd.to_datetime(str(v).strip(), dayfirst=True, errors=\"coerce\")\n",
    "    return None if pd.isna(dt) else dt.date()\n",
    "        \n",
    "def _cv_id(conn, cv_partner_cv_id: str):\n",
    "    return conn.execute(\n",
    "        text(\"SELECT cv_id FROM cvs WHERE cv_partner_cv_id=:cid\"),\n",
    "        {\"cid\": str(cv_partner_cv_id)}\n",
    "    ).scalar()\n",
    "\n",
    "def _ensure_dim(conn, table: str, name: str, key: str = \"name\", id_col: str = None):\n",
    "    if not name:\n",
    "        return None\n",
    "    if id_col is None:\n",
    "        id_col = (table[4:] + \"_id\") if table.startswith(\"dim_\") else (table.rstrip(\"s\") + \"_id\")\n",
    "    conn.execute(text(f\"INSERT INTO {table} ({key}) VALUES (:n) ON CONFLICT ({key}) DO NOTHING\"), {\"n\": name})\n",
    "    return conn.execute(text(f\"SELECT {id_col} FROM {table} WHERE {key}=:n\"), {\"n\": name}).scalar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e33021",
   "metadata": {},
   "source": [
    "## Step 3.2 — Core entity upserts (Users and CVs)\n",
    "\n",
    "These upsert functions populate the `users` and `cvs` tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea8a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_users(conn, df):\n",
    "    print(f\"Upserting {len(df)} users.\")\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO users\n",
    "          (cv_partner_user_id, name_multilang, email, upn, external_user_id,\n",
    "           phone_number, landline, birth_year, department, country,\n",
    "           user_created_at, nationality_multilang)\n",
    "        VALUES\n",
    "          (:cv_partner_user_id, CAST(:name_multilang AS JSONB), :email, :upn, :external_user_id,\n",
    "           :phone_number, :landline, :birth_year, :department, :country,\n",
    "           :user_created_at, CAST(:nationality_multilang AS JSONB))\n",
    "        ON CONFLICT (cv_partner_user_id) DO UPDATE\n",
    "        SET name_multilang = EXCLUDED.name_multilang,\n",
    "            email = EXCLUDED.email,\n",
    "            upn = EXCLUDED.upn,\n",
    "            external_user_id = EXCLUDED.external_user_id,\n",
    "            phone_number = EXCLUDED.phone_number,\n",
    "            landline = EXCLUDED.landline,\n",
    "            birth_year = EXCLUDED.birth_year,\n",
    "            department = EXCLUDED.department,\n",
    "            country = EXCLUDED.country,\n",
    "            user_created_at = EXCLUDED.user_created_at,\n",
    "            nationality_multilang = EXCLUDED.nationality_multilang\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        conn.execute(sql, {\n",
    "            \"cv_partner_user_id\": str(r[\"CV Partner User ID\"]),\n",
    "            \"name_multilang\": json.dumps(r[\"Name (multilang)\"]),  # dict -> JSON\n",
    "            \"email\": r.get(\"Email\"),\n",
    "            \"upn\": r.get(\"UPN\"),\n",
    "            \"external_user_id\": r.get(\"External User ID\"),\n",
    "            \"phone_number\": r.get(\"Phone Number\"),\n",
    "            \"landline\": r.get(\"Landline\"),\n",
    "            \"birth_year\": int(r[\"Birth Year\"]) if pd.notna(r.get(\"Birth Year\")) else None,\n",
    "            \"department\": r.get(\"Department\"),\n",
    "            \"country\": r.get(\"Country\"),\n",
    "            \"user_created_at\": r.get(\"User created at\"),\n",
    "            \"nationality_multilang\": json.dumps(r.get(\"nationality_multilang\", {})),\n",
    "        })\n",
    "\n",
    "def upsert_cvs(conn, df):\n",
    "    print(f\"Upserting {len(df)} CVs...\")\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO cvs\n",
    "          (cv_partner_cv_id, user_id, title_multilang, years_of_education,\n",
    "           years_since_first_work_experience, has_profile_image,\n",
    "           owns_reference_project, read_privacy_notice,\n",
    "           cv_last_updated_by_owner, cv_last_updated,\n",
    "           sfia_level, cpd_level, cpd_band, cpd_label)\n",
    "        VALUES\n",
    "          (:cv_partner_cv_id, :user_id, CAST(:title_multilang AS JSONB), :yoe, :ysfwe,\n",
    "           :has_img, :owns_ref, :read_priv, :lu_owner, :lu,\n",
    "           :sfia_level, :cpd_level, :cpd_band, :cpd_label)\n",
    "        ON CONFLICT (cv_partner_cv_id) DO UPDATE\n",
    "        SET title_multilang = EXCLUDED.title_multilang,\n",
    "            years_of_education = EXCLUDED.years_of_education,\n",
    "            years_since_first_work_experience = EXCLUDED.years_since_first_work_experience,\n",
    "            has_profile_image = EXCLUDED.has_profile_image,\n",
    "            owns_reference_project = EXCLUDED.owns_reference_project,\n",
    "            read_privacy_notice = EXCLUDED.read_privacy_notice,\n",
    "            cv_last_updated_by_owner = EXCLUDED.cv_last_updated_by_owner,\n",
    "            cv_last_updated = EXCLUDED.cv_last_updated,\n",
    "            sfia_level = EXCLUDED.sfia_level,\n",
    "            cpd_level  = EXCLUDED.cpd_level,\n",
    "            cpd_band   = EXCLUDED.cpd_band,\n",
    "            cpd_label  = EXCLUDED.cpd_label\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        uid = conn.execute(\n",
    "            text(\"SELECT user_id FROM users WHERE cv_partner_user_id=:uid\"),\n",
    "            {\"uid\": str(r[\"CV Partner User ID\"])}\n",
    "        ).scalar()\n",
    "        if uid is None:\n",
    "            print(f\"  ⚠️ Skipping CV {r['CV Partner CV ID']} (unknown user {r['CV Partner User ID']})\")\n",
    "            continue\n",
    "\n",
    "        conn.execute(sql, {\n",
    "            \"cv_partner_cv_id\": str(r[\"CV Partner CV ID\"]),\n",
    "            \"user_id\": uid,\n",
    "            \"title_multilang\": json.dumps(r[\"title_multilang\"]),\n",
    "            \"yoe\": int(r[\"Years of education\"]) if pd.notna(r[\"Years of education\"]) else None,\n",
    "            \"ysfwe\": int(r[\"Years since first work experience\"]) if pd.notna(r[\"Years since first work experience\"]) else None,\n",
    "            \"has_img\": _to_bool(r[\"Has profile image\"]),\n",
    "            \"owns_ref\": _to_bool(r[\"Owns a reference project\"]),\n",
    "            \"read_priv\": _to_bool(r[\"Read and understood privacy notice\"]),\n",
    "            \"lu_owner\": r[\"CV Last updated by owner\"],\n",
    "            \"lu\": r[\"CV Last updated\"],\n",
    "            \"sfia_level\": r.get(\"sfia_level\"),\n",
    "            \"cpd_level\":  r.get(\"cpd_level\"),\n",
    "            \"cpd_band\":   (None if pd.isna(r.get(\"cpd_band\"))  else str(r.get(\"cpd_band\"))),\n",
    "            \"cpd_label\":  (None if pd.isna(r.get(\"cpd_label\")) else str(r.get(\"cpd_label\"))),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b25529",
   "metadata": {},
   "source": [
    "## Step 3.3 — Skills and languages\n",
    "\n",
    "Upserts for technology skills, languages, and related dimension tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67306ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_technologies(conn, df):\n",
    "    print(f\"Upserting {len(df)} technologies...\")\n",
    "    for _, r in df.iterrows():\n",
    "        tech_name = r[\"Skill name\"]\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO dim_technology (name)\n",
    "            VALUES (:name)\n",
    "            ON CONFLICT (name) DO NOTHING\n",
    "        \"\"\"), {\"name\": tech_name})\n",
    "\n",
    "        tech_id = conn.execute(\n",
    "            text(\"SELECT technology_id FROM dim_technology WHERE name=:n\"),\n",
    "            {\"n\": tech_name}\n",
    "        ).scalar()\n",
    "        if tech_id is None:\n",
    "            print(f\"  ⚠️ Skipping tech link; cannot resolve technology '{tech_name}'\")\n",
    "            continue\n",
    "\n",
    "        cv_id = conn.execute(\n",
    "            text(\"SELECT cv_id FROM cvs WHERE cv_partner_cv_id=:cid\"),\n",
    "            {\"cid\": str(r[\"CV Partner CV ID\"])}\n",
    "        ).scalar()\n",
    "        if cv_id is None:\n",
    "            print(f\"  ⚠️ Skipping tech link; unknown CV {r['CV Partner CV ID']}\")\n",
    "            continue\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO cv_technology (cv_id, technology_id, years_experience, proficiency, is_official_masterdata)\n",
    "            VALUES (:cv, :tech, :yexp, :prof, CAST(:is_md AS JSONB))\n",
    "            ON CONFLICT (cv_id, technology_id) DO UPDATE\n",
    "            SET years_experience = EXCLUDED.years_experience,\n",
    "                proficiency = EXCLUDED.proficiency,\n",
    "                is_official_masterdata = EXCLUDED.is_official_masterdata\n",
    "        \"\"\"), {\n",
    "            \"cv\": cv_id,\n",
    "            \"tech\": tech_id,\n",
    "            \"yexp\": int(r[\"Year experience\"]) if pd.notna(r[\"Year experience\"]) else None,\n",
    "            \"prof\": int(r[\"Proficiency (0-5)\"]) if pd.notna(r[\"Proficiency (0-5)\"]) else None,\n",
    "            \"is_md\": json.dumps(r[\"Is official masterdata (in #{lang})\"])  # dict -> json\n",
    "        })\n",
    "\n",
    "def upsert_languages(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} languages...\")\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO cv_language\n",
    "          (cv_id, language_id, level, highlighted, is_official_masterdata, updated, updated_by_owner)\n",
    "        VALUES\n",
    "          (:cv_id, :lang_id, :level, :highlighted, CAST(:is_md AS JSONB), :updated, :updated_by_owner)\n",
    "        ON CONFLICT (cv_id, language_id) DO UPDATE\n",
    "        SET level = EXCLUDED.level,\n",
    "            highlighted = EXCLUDED.highlighted,\n",
    "            is_official_masterdata = EXCLUDED.is_official_masterdata,\n",
    "            updated = EXCLUDED.updated,\n",
    "            updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        lang_id = _ensure_dim(conn, \"dim_language\", r.get(\"Language\"))\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"lang_id\": lang_id,\n",
    "            \"level\": r.get(\"Level\"),\n",
    "            \"highlighted\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"is_md\": json.dumps(r.get(\"Is official masterdata (in #{lang})\", {})),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ff34cc",
   "metadata": {},
   "source": [
    "## Step 3.4 — Project experience, work experience, certifications, courses, education, and positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13e851d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upsert_project_experiences(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} project experiences...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO project_experience\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month_from, year_from, month_to, year_to,\n",
    "         customer_int, customer_multilang,\n",
    "         customer_anon_int, customer_anon_multilang,\n",
    "         description_int, description_multilang,\n",
    "         long_description_int, long_description_multilang,\n",
    "         industry_id, project_type_id,\n",
    "         percent_allocated, extent_individual_hours, extent_hours, extent_total_hours,\n",
    "         extent_unit, extent_currency, extent_total, extent_total_currency,\n",
    "         project_area, project_area_unit,\n",
    "         highlighted, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id,\n",
    "         :m_from, :y_from, :m_to, :y_to,\n",
    "         :cust_int, CAST(:cust_ml AS JSONB),\n",
    "         :cust_anon_int, CAST(:cust_anon_ml AS JSONB),\n",
    "         :desc_int, CAST(:desc_ml AS JSONB),\n",
    "         :ldesc_int, CAST(:ldesc_ml AS JSONB),\n",
    "         :industry_id, :project_type_id,\n",
    "         :pct_alloc, :indiv_hours, :hours, :total_hours,\n",
    "         :extent_unit, :extent_curr, :extent_total, :extent_total_curr,\n",
    "         :proj_area, :proj_area_unit,\n",
    "         :highlighted, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month_from = EXCLUDED.month_from, year_from = EXCLUDED.year_from,\n",
    "          month_to = EXCLUDED.month_to, year_to = EXCLUDED.year_to,\n",
    "          customer_int = EXCLUDED.customer_int, customer_multilang = EXCLUDED.customer_multilang,\n",
    "          customer_anon_int = EXCLUDED.customer_anon_int, customer_anon_multilang = EXCLUDED.customer_anon_multilang,\n",
    "          description_int = EXCLUDED.description_int, description_multilang = EXCLUDED.description_multilang,\n",
    "          long_description_int = EXCLUDED.long_description_int, long_description_multilang = EXCLUDED.long_description_multilang,\n",
    "          industry_id = EXCLUDED.industry_id, project_type_id = EXCLUDED.project_type_id,\n",
    "          percent_allocated = EXCLUDED.percent_allocated,\n",
    "          extent_individual_hours = EXCLUDED.extent_individual_hours,\n",
    "          extent_hours = EXCLUDED.extent_hours,\n",
    "          extent_total_hours = EXCLUDED.extent_total_hours,\n",
    "          extent_unit = EXCLUDED.extent_unit,\n",
    "          extent_currency = EXCLUDED.extent_currency,\n",
    "          extent_total = EXCLUDED.extent_total,\n",
    "          extent_total_currency = EXCLUDED.extent_total_currency,\n",
    "          project_area = EXCLUDED.project_area, project_area_unit = EXCLUDED.project_area_unit,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        industry_id = _ensure_dim(conn, \"dim_industry\", r.get(\"Industry (int)\"))\n",
    "        projtype_id = _ensure_dim(conn, \"dim_project_type\", r.get(\"Project type (int)\"))\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m_from\": r.get(\"Month from\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"m_to\": r.get(\"Month to\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"cust_int\": r.get(\"Customer (int)\"),\n",
    "            \"cust_ml\": json.dumps(r.get(\"Customer (#{lang})\", {})),\n",
    "            \"cust_anon_int\": r.get(\"Customer Anonymized (int)\"),\n",
    "            \"cust_anon_ml\": json.dumps(r.get(\"Customer Anonymized (#{lang})\", {})),\n",
    "            \"desc_int\": r.get(\"Description (int)\"),\n",
    "            \"desc_ml\": json.dumps(r.get(\"Description (#{lang})\", {})),\n",
    "            \"ldesc_int\": r.get(\"Long description (int)\"),\n",
    "            \"ldesc_ml\": json.dumps(r.get(\"Long description (#{lang})\", {})),\n",
    "            \"industry_id\": industry_id,\n",
    "            \"project_type_id\": projtype_id,\n",
    "            \"pct_alloc\": r.get(\"Percent allocated\"),\n",
    "            \"indiv_hours\": r.get(\"Project extent (individual hours)\"),\n",
    "            \"hours\": r.get(\"Project extent (hours)\"),\n",
    "            \"total_hours\": r.get(\"Project extent total (hours)\"),\n",
    "            \"extent_unit\": r.get(\"Project extent\"),\n",
    "            \"extent_curr\": r.get(\"Project extent (currency)\"),\n",
    "            \"extent_total\": r.get(\"Project extent total\"),\n",
    "            \"extent_total_curr\": r.get(\"Project extent total (currency)\"),\n",
    "            \"proj_area\": r.get(\"Project area\"),\n",
    "            \"proj_area_unit\": r.get(\"Project area (unit)\"),\n",
    "            \"highlighted\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_work_experiences(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} work experiences...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO work_experience\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month_from, year_from, month_to, year_to,\n",
    "         highlighted, employer, description, long_description,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id,\n",
    "         :m_from, :y_from, :m_to, :y_to,\n",
    "         :highlighted, :employer, :desc, :ldesc,\n",
    "         :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month_from = EXCLUDED.month_from, year_from = EXCLUDED.year_from,\n",
    "          month_to = EXCLUDED.month_to, year_to = EXCLUDED.year_to,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          employer = EXCLUDED.employer,\n",
    "          description = EXCLUDED.description,\n",
    "          long_description = EXCLUDED.long_description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m_from\": r.get(\"Month from\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"m_to\": r.get(\"Month to\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"highlighted\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"employer\": r.get(\"Employer\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"ldesc\": r.get(\"Long Description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_certifications(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} certifications...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO certification\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month, year, month_expire, year_expire,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :m, :y, :mexp, :yexp, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month = EXCLUDED.month, year = EXCLUDED.year,\n",
    "          month_expire = EXCLUDED.month_expire, year_expire = EXCLUDED.year_expire,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m\": r.get(\"Month\"),\n",
    "            \"y\": r.get(\"Year\"),\n",
    "            \"mexp\": r.get(\"Month expire\"),\n",
    "            \"yexp\": r.get(\"Year expire\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_courses(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} courses...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO course\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month, year, name, organiser, long_description, highlighted,\n",
    "         is_official_masterdata, attachments, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :m, :y, :name, :org, :ldesc, :hl,\n",
    "         CAST(:is_md AS JSONB), :att, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month = EXCLUDED.month, year = EXCLUDED.year,\n",
    "          name = EXCLUDED.name, organiser = EXCLUDED.organiser,\n",
    "          long_description = EXCLUDED.long_description,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          is_official_masterdata = EXCLUDED.is_official_masterdata,\n",
    "          attachments = EXCLUDED.attachments,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m\": r.get(\"Month\"),\n",
    "            \"y\": r.get(\"Year\"),\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"org\": r.get(\"Organiser\"),\n",
    "            \"ldesc\": r.get(\"Long description\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"is_md\": json.dumps(r.get(\"Is official masterdata (in #{lang})\", {})),\n",
    "            \"att\": r.get(\"Attachments\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_educations(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} educations...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO education\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         month_from, year_from, month_to, year_to,\n",
    "         highlighted, attachments, place_of_study, degree, description,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id,\n",
    "         :m_from, :y_from, :m_to, :y_to,\n",
    "         :hl, :att, :place, :deg, :desc,\n",
    "         :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          month_from = EXCLUDED.month_from, year_from = EXCLUDED.year_from,\n",
    "          month_to = EXCLUDED.month_to, year_to = EXCLUDED.year_to,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          attachments = EXCLUDED.attachments,\n",
    "          place_of_study = EXCLUDED.place_of_study,\n",
    "          degree = EXCLUDED.degree,\n",
    "          description = EXCLUDED.description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"m_from\": r.get(\"Month from\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"m_to\": r.get(\"Month to\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"att\": r.get(\"Attachments\"),\n",
    "            \"place\": r.get(\"Place of study\"),\n",
    "            \"deg\": r.get(\"Degree\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_positions(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} positions...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO position\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         year_from, year_to, highlighted, name, description,\n",
    "         updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :y_from, :y_to, :hl, :name, :desc, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          year_from = EXCLUDED.year_from, year_to = EXCLUDED.year_to,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          name = EXCLUDED.name, description = EXCLUDED.description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"y_from\": r.get(\"Year from\"),\n",
    "            \"y_to\": r.get(\"Year to\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_blogs(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} blogs/publications...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO blog_publication\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         name, description, highlighted, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :name, :desc, :hl, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          name = EXCLUDED.name, description = EXCLUDED.description,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_cv_roles(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} cv roles...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO cv_role\n",
    "        (cv_id, name, description, highlighted, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :name, :desc, :hl, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, name) DO UPDATE\n",
    "      SET description = EXCLUDED.description,\n",
    "          highlighted = EXCLUDED.highlighted,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"name\": r.get(\"Name\"),\n",
    "            \"desc\": r.get(\"Description\"),\n",
    "            \"hl\": _to_bool(r.get(\"Highlighted\")),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })\n",
    "\n",
    "def upsert_key_qualifications(conn, df):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    print(f\"Upserting {len(df)} key qualifications...\")\n",
    "    sql = text(\"\"\"\n",
    "      INSERT INTO key_qualification\n",
    "        (cv_id, cv_partner_section_id, external_unique_id,\n",
    "         label, summary, short_description, updated, updated_by_owner)\n",
    "      VALUES\n",
    "        (:cv_id, :sid, :ext_id, :label, :summary, :short_desc, :updated, :updated_by_owner)\n",
    "      ON CONFLICT (cv_id, cv_partner_section_id) DO UPDATE\n",
    "      SET external_unique_id = EXCLUDED.external_unique_id,\n",
    "          label = EXCLUDED.label, summary = EXCLUDED.summary,\n",
    "          short_description = EXCLUDED.short_description,\n",
    "          updated = EXCLUDED.updated, updated_by_owner = EXCLUDED.updated_by_owner\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        cv_id = _cv_id(conn, r[\"CV Partner CV ID\"])\n",
    "        if not cv_id:\n",
    "            continue\n",
    "        conn.execute(sql, {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"sid\": r.get(\"CV Partner section ID\"),\n",
    "            \"ext_id\": r.get(\"External unique ID\"),\n",
    "            \"label\": r.get(\"Label\"),\n",
    "            \"summary\": r.get(\"Summary of Qualifications\"),\n",
    "            \"short_desc\": r.get(\"Short description\"),\n",
    "            \"updated\": r.get(\"Updated\"),\n",
    "            \"updated_by_owner\": r.get(\"Updated by owner\"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac67bad",
   "metadata": {},
   "source": [
    "## Step 3.5 — Security clearance and availability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d092d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_sc_clearance(conn, df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    for _, r in df.iterrows():\n",
    "        uid = _resolve_user_id(conn, r.get(\"Email\"), r.get(\"UPN\"), r.get(\"External User ID\"))\n",
    "        if not uid:\n",
    "            continue\n",
    "\n",
    "        clr = _clean_str(r.get(\"Clearance\"), \"None\") or \"None\"\n",
    "        conn.execute(text(\"INSERT INTO dim_clearance(name) VALUES (:n) ON CONFLICT(name) DO NOTHING\"),\n",
    "                     {\"n\": clr})\n",
    "        clr_id = conn.execute(text(\"SELECT clearance_id FROM dim_clearance WHERE name=:n\"),\n",
    "                              {\"n\": clr}).scalar()\n",
    "\n",
    "        # Default valid_from if missing so we never violate NOT NULL\n",
    "        vf = _to_date(r.get(\"Valid From\"), default=date(1900, 1, 1))\n",
    "        vt = _to_date(r.get(\"Valid To\"))\n",
    "        vb = _clean_str(r.get(\"Verified By\"), None) or None\n",
    "        no = _clean_str(r.get(\"Notes\"), None) or None\n",
    "\n",
    "        # If both present and vt < vf (bad data), drop vt\n",
    "        if vt and vf and vt < vf:\n",
    "            vt = None\n",
    "\n",
    "        conn.execute(text(\"\"\"\n",
    "            INSERT INTO user_clearance(user_id, clearance_id, valid_from, valid_to, verified_by, notes)\n",
    "            VALUES (:u, :c, :vf, :vt, :vb, :no)\n",
    "            ON CONFLICT (user_id, clearance_id, valid_from) DO UPDATE\n",
    "            SET valid_to    = EXCLUDED.valid_to,\n",
    "                verified_by = EXCLUDED.verified_by,\n",
    "                notes       = EXCLUDED.notes\n",
    "        \"\"\"), {\"u\": uid, \"c\": clr_id, \"vf\": vf, \"vt\": vt, \"vb\": vb, \"no\": no})\n",
    "\n",
    "\n",
    "\n",
    "def upsert_availability(conn, df: pd.DataFrame):\n",
    "    if df is None or df.empty:\n",
    "        return\n",
    "    sql = text(\"\"\"\n",
    "        INSERT INTO user_availability(user_id, date, percent_available, source)\n",
    "        VALUES (:u, :d, :p, :s)\n",
    "        ON CONFLICT (user_id, date) DO UPDATE\n",
    "        SET percent_available = EXCLUDED.percent_available,\n",
    "            source            = EXCLUDED.source,\n",
    "            updated_at        = NOW()\n",
    "    \"\"\")\n",
    "    for _, r in df.iterrows():\n",
    "        uid = _resolve_user_id(conn, r.get(\"Email\"), r.get(\"UPN\"), r.get(\"External User ID\"))\n",
    "        if not uid:\n",
    "            continue\n",
    "        # percent can come as float/NaN — clamp to [0,100]\n",
    "        raw = r.get(\"Percent Available\")\n",
    "        p = 0 if (raw is None or (isinstance(raw, float) and pd.isna(raw))) else int(float(raw))\n",
    "        p = max(0, min(100, p))\n",
    "        conn.execute(sql, {\n",
    "            \"u\": uid,\n",
    "            \"d\": _clean_str(r.get(\"Date\"), None) or None,\n",
    "            \"p\": p,\n",
    "            \"s\": _clean_str(r.get(\"Source\"), \"Fake generator\"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d26263",
   "metadata": {},
   "source": [
    "## Step 3.6 — Load orchestrator\n",
    "\n",
    "The `load()` function runs all upserts in the correct sequence inside a \n",
    "single database transaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b8135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple, clean load implementation below ---\n",
    "def load(clean_data, engine):\n",
    "    \"\"\"\n",
    "    Loads each cleaned DataFrame into the database using upsert logic.\n",
    "    \"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        if getattr(clean_data, 'users_df', None) is not None:\n",
    "            upsert_users(conn, clean_data.users_df)\n",
    "        if getattr(clean_data, 'cvs_df', None) is not None:\n",
    "            upsert_cvs(conn, clean_data.cvs_df)\n",
    "        if getattr(clean_data, 'technologies_df', None) is not None:\n",
    "            upsert_technologies(conn, clean_data.technologies_df)\n",
    "        if getattr(clean_data, 'languages_df', None) is not None:\n",
    "            upsert_languages(conn, clean_data.languages_df)\n",
    "        if getattr(clean_data, 'project_experiences_df', None) is not None:\n",
    "            upsert_project_experiences(conn, clean_data.project_experiences_df)\n",
    "        if getattr(clean_data, 'work_experiences_df', None) is not None:\n",
    "            upsert_work_experiences(conn, clean_data.work_experiences_df)\n",
    "        if getattr(clean_data, 'certifications_df', None) is not None:\n",
    "            upsert_certifications(conn, clean_data.certifications_df)\n",
    "        if getattr(clean_data, 'courses_df', None) is not None:\n",
    "            upsert_courses(conn, clean_data.courses_df)\n",
    "        if getattr(clean_data, 'educations_df', None) is not None:\n",
    "            upsert_educations(conn, clean_data.educations_df)\n",
    "        if getattr(clean_data, 'positions_df', None) is not None:\n",
    "            upsert_positions(conn, clean_data.positions_df)\n",
    "        if getattr(clean_data, 'blogs_df', None) is not None:\n",
    "            upsert_blogs(conn, clean_data.blogs_df)\n",
    "        if getattr(clean_data, 'cv_roles_df', None) is not None:\n",
    "            upsert_cv_roles(conn, clean_data.cv_roles_df)\n",
    "        if getattr(clean_data, 'key_qualifications_df', None) is not None:\n",
    "            upsert_key_qualifications(conn, clean_data.key_qualifications_df)\n",
    "        if getattr(clean_data, 'sc_clearance_df', None) is not None:\n",
    "            upsert_sc_clearance(conn, clean_data.sc_clearance_df)\n",
    "        if getattr(clean_data, 'availability_df', None) is not None:\n",
    "            upsert_availability(conn, clean_data.availability_df)\n",
    "    print(\"✅ Load complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c025fc2e",
   "metadata": {},
   "source": [
    "# Step 4 — Database Setup\n",
    "\n",
    "These steps configure PostgreSQL connection settings, create the database if \n",
    "missing, and apply schema files from `sql/*.sql`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8be3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db_config_txt(path=\"db_config.txt\"):\n",
    "    db = {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if \"=\" in line:\n",
    "                    k, v = line.strip().split(\"=\", 1)\n",
    "                    db[k.strip()] = v.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return db\n",
    "\n",
    "def compose_settings():\n",
    "    # precedence: ENV > db_config.txt > defaults\n",
    "    defaults = dict(host=\"localhost\", port=5432, database=\"flowcase_demo\",\n",
    "                    user=\"postgres\", password=\"postgres\")\n",
    "    file_cfg = read_db_config_txt()\n",
    "\n",
    "    env_cfg = dict(\n",
    "        host=os.getenv(\"PGHOST\"),\n",
    "        port=os.getenv(\"PGPORT\"),\n",
    "        database=os.getenv(\"PGDATABASE\"),\n",
    "        user=os.getenv(\"PGUSER\"),\n",
    "        password=os.getenv(\"PGPASSWORD\"),\n",
    "    )\n",
    "    # drop Nones\n",
    "    env_cfg = {k:v for k,v in env_cfg.items() if v is not None}\n",
    "    # coerce port\n",
    "    if \"port\" in env_cfg:\n",
    "        try: env_cfg[\"port\"] = int(env_cfg[\"port\"])\n",
    "        except: env_cfg.pop(\"port\", None)\n",
    "\n",
    "    db = {**defaults, **file_cfg, **env_cfg}\n",
    "\n",
    "    settings = {\n",
    "        \"data_source\": \"fake\",\n",
    "        \"base_folder\": \"cv_reports\",\n",
    "        \"db\": db,\n",
    "        # let utils apply ALL sql/*.sql automatically\n",
    "        \"schema\": {\"apply_all_sql_in_sql_folder\": True, \"folder\": \"sql\"}\n",
    "    }\n",
    "    return settings\n",
    "\n",
    "def ensure_database_exists(db):\n",
    "    try:\n",
    "        conn = psycopg2.connect(dbname=\"postgres\",\n",
    "                                user=db[\"user\"], password=db[\"password\"],\n",
    "                                host=db[\"host\"], port=db[\"port\"])\n",
    "        conn.autocommit = True\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT 1 FROM pg_database WHERE datname = %s\", (db[\"database\"],))\n",
    "        if not cur.fetchone():\n",
    "            print(f\"Database '{db['database']}' does not exist. Creating...\")\n",
    "            cur.execute(f\"CREATE DATABASE {db['database']};\")\n",
    "        cur.close(); conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not check/create database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0673d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_database_engine(db_settings: dict):\n",
    "    url = (\n",
    "        \"postgresql+psycopg2://\"\n",
    "        f\"{db_settings['user']}:{db_settings['password']}\"\n",
    "        f\"@{db_settings['host']}:{db_settings['port']}/{db_settings['database']}\"\n",
    "    )\n",
    "    return create_engine(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7c76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_database_schema_if_needed(engine, settings: dict):\n",
    "    schema_cfg = settings.get(\"schema\", {})\n",
    "    if not schema_cfg.get(\"apply_all_sql_in_sql_folder\"):\n",
    "        return\n",
    "\n",
    "    sql_folder = Path(schema_cfg.get(\"folder\", \"sql\"))\n",
    "    if not sql_folder.exists():\n",
    "        print(f\"Schema folder {sql_folder} does not exist. Skipping schema setup.\")\n",
    "        return\n",
    "\n",
    "    with engine.begin() as conn:\n",
    "        for path in sorted(sql_folder.glob(\"*.sql\")):\n",
    "            print(f\"Applying schema from {path.name}...\")\n",
    "            conn.execute(text(path.read_text()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a489cd8",
   "metadata": {},
   "source": [
    "# Step 5 — Run Full ETL Pipeline\n",
    "\n",
    "This executes:\n",
    "\n",
    "1. Extract  \n",
    "2. Transform  \n",
    "3. Load  \n",
    "\n",
    "And confirms everything worked end-to-end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b91cda42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying schema from 01_schema.sql...\n",
      "Applying schema from 02_cv_search_profile_mv.sql...\n",
      "\n",
      "==================== Finding the latest quarterly report folder ====================\n",
      "Quarterly folders found: ['Q42025']\n",
      "Using latest quarterly folder: Q42025\n",
      "\n",
      "==================== Loading CSV files from cv_reports/Q42025 ====================\n",
      "Found 15 CSV files.\n",
      "  Loaded certifications.csv -> (1006, 22)\n",
      "  Loaded project_experiences.csv -> (1501, 45)\n",
      "  Loaded blogs.csv -> (758, 21)\n",
      "  Loaded availability_report.csv -> (30000, 7)\n",
      "  Loaded cv_roles.csv -> (1001, 19)\n",
      "  Loaded work_experiences.csv -> (1484, 26)\n",
      "  Loaded educations.csv -> (757, 27)\n",
      "  Loaded user_report.csv -> (500, 26)\n",
      "  Loaded courses.csv -> (1446, 26)\n",
      "  Loaded key_qualifications.csv -> (485, 21)\n",
      "  Loaded positions.csv -> (1264, 23)\n",
      "  Loaded technologies.csv -> (2220, 20)\n",
      "  Loaded sc_clearance.csv -> (500, 9)\n",
      "  Loaded languages.csv -> (996, 22)\n",
      "  Loaded usage_report.csv -> (500, 51)\n",
      "Using data folder: cv_reports/Q42025\n",
      "Upserting 500 users.\n",
      "Upserting 500 CVs...\n",
      "Upserting 2220 technologies...\n",
      "Upserting 996 languages...\n",
      "Upserting 1501 project experiences...\n",
      "Upserting 1484 work experiences...\n",
      "Upserting 1006 certifications...\n",
      "Upserting 1446 courses...\n",
      "Upserting 757 educations...\n",
      "Upserting 1264 positions...\n",
      "Upserting 758 blogs/publications...\n",
      "Upserting 1001 cv roles...\n",
      "Upserting 485 key qualifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3p/rr2nrbcj6j79v6mwqtwtdzbw0000gn/T/ipykernel_55247/3417465587.py:32: UserWarning: Parsing dates in %Y-%m-%d format when dayfirst=True was specified. Pass `dayfirst=False` or specify a format to silence this warning.\n",
      "  dt = pd.to_datetime(str(v).strip(), dayfirst=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Load complete.\n",
      "✅ Flowcase ETL (manual fake) complete.\n"
     ]
    }
   ],
   "source": [
    "settings = compose_settings()\n",
    "db_settings = settings[\"db\"]\n",
    "\n",
    "ensure_database_exists(db_settings)\n",
    "engine = create_database_engine(db_settings)\n",
    "\n",
    "# 🔹 New line: apply schema from sql/*.sql\n",
    "setup_database_schema_if_needed(engine, settings)\n",
    "\n",
    "ex = extract(settings)\n",
    "print(f\"Using data folder: {getattr(ex, 'data_dir', 'unknown')}\")\n",
    "tr = transform(ex)\n",
    "\n",
    "load(tr, engine)\n",
    "print(\"✅ Flowcase ETL (manual fake) complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b25f8",
   "metadata": {},
   "source": [
    "## Step 5.1 — Basic database verification\n",
    "\n",
    "After running the full ETL, we check that key tables contain data as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ac85034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users in DB: 500\n",
      "CVs in DB: 500\n",
      "CV–technology links in DB: 2220\n",
      "✅ Basic load checks passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with engine.connect() as conn:\n",
    "    users_count = conn.execute(text(\"SELECT COUNT(*) FROM users\")).scalar()\n",
    "    cvs_count = conn.execute(text(\"SELECT COUNT(*) FROM cvs\")).scalar()\n",
    "    tech_links = conn.execute(text(\"SELECT COUNT(*) FROM cv_technology\")).scalar()\n",
    "\n",
    "print(f\"Users in DB: {users_count}\")\n",
    "print(f\"CVs in DB: {cvs_count}\")\n",
    "print(f\"CV–technology links in DB: {tech_links}\")\n",
    "\n",
    "assert users_count > 0, \"No users loaded!\"\n",
    "assert cvs_count > 0, \"No CVs loaded!\"\n",
    "print(\"✅ Basic load checks passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b3dc2a",
   "metadata": {},
   "source": [
    "# Step 6 - materialised search view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1626084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 6 — Refresh materialised search view ====================\n",
      "\n",
      "Sample rows from cv_search_profile_mv:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>cv_partner_user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>cv_id</th>\n",
       "      <th>cv_title</th>\n",
       "      <th>sfia_level</th>\n",
       "      <th>cpd_label</th>\n",
       "      <th>technologies</th>\n",
       "      <th>max_years_experience</th>\n",
       "      <th>clearance</th>\n",
       "      <th>latest_availability_date</th>\n",
       "      <th>latest_percent_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b89ca31a</td>\n",
       "      <td>Danielle Johnson</td>\n",
       "      <td>1</td>\n",
       "      <td>Principal C# Developer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>.NET, Azure, Kubernetes, Python, dbt</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>74b836a3</td>\n",
       "      <td>Joshua Walker</td>\n",
       "      <td>2</td>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>C#, Kafka, Node.js, Snowflake, dbt</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70aaa8ab</td>\n",
       "      <td>Jill Rhodes</td>\n",
       "      <td>3</td>\n",
       "      <td>Senior C# Developer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>.NET, Node.js, React, Snowflake, TypeScript, dbt</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>080ef439</td>\n",
       "      <td>Patricia Miller</td>\n",
       "      <td>4</td>\n",
       "      <td>Principal Analytics Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>Airflow, C#, JavaScript, Kafka, Spark, Terraform</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4253cf51</td>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>5</td>\n",
       "      <td>Associate ML Engineer</td>\n",
       "      <td>2</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>Airflow, Node.js, Python, Spark</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6b7c67d6</td>\n",
       "      <td>Jeffery Wagner</td>\n",
       "      <td>6</td>\n",
       "      <td>Consultant Data Platform Engineer</td>\n",
       "      <td>3</td>\n",
       "      <td>CPD2L</td>\n",
       "      <td>Airflow, JavaScript, Power BI, dbt</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1266d379</td>\n",
       "      <td>Anthony Gonzalez</td>\n",
       "      <td>7</td>\n",
       "      <td>Principal Azure Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>C#, Databricks, SQL, Snowflake, Terraform</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>f81d901a</td>\n",
       "      <td>Debra Gardner</td>\n",
       "      <td>8</td>\n",
       "      <td>Senior AI Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>Airflow, JavaScript, Oracle, Power BI, SQL, Te...</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>27b5fdea</td>\n",
       "      <td>Jeffrey Lawrence</td>\n",
       "      <td>9</td>\n",
       "      <td>Associate Backend Engineer</td>\n",
       "      <td>2</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>AWS, Azure, C#, Oracle, Power BI, TypeScript</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8eaaedc9</td>\n",
       "      <td>Lisa Smith</td>\n",
       "      <td>10</td>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>.NET, Airflow, Azure, React, Snowflake</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id cv_partner_user_id         user_name  cv_id  \\\n",
       "0        1           b89ca31a  Danielle Johnson      1   \n",
       "1        2           74b836a3     Joshua Walker      2   \n",
       "2        3           70aaa8ab       Jill Rhodes      3   \n",
       "3        4           080ef439   Patricia Miller      4   \n",
       "4        5           4253cf51    Robert Johnson      5   \n",
       "5        6           6b7c67d6    Jeffery Wagner      6   \n",
       "6        7           1266d379  Anthony Gonzalez      7   \n",
       "7        8           f81d901a     Debra Gardner      8   \n",
       "8        9           27b5fdea  Jeffrey Lawrence      9   \n",
       "9       10           8eaaedc9        Lisa Smith     10   \n",
       "\n",
       "                            cv_title  sfia_level cpd_label  \\\n",
       "0             Principal C# Developer           5     CPD3L   \n",
       "1            Principal Data Engineer           5     CPD3L   \n",
       "2                Senior C# Developer           4     CPD3E   \n",
       "3       Principal Analytics Engineer           5     CPD3L   \n",
       "4              Associate ML Engineer           2     CPD1E   \n",
       "5  Consultant Data Platform Engineer           3     CPD2L   \n",
       "6           Principal Azure Engineer           5     CPD3L   \n",
       "7                 Senior AI Engineer           4     CPD3E   \n",
       "8         Associate Backend Engineer           2     CPD1E   \n",
       "9           Associate Data Scientist           2     CPD1E   \n",
       "\n",
       "                                        technologies  max_years_experience  \\\n",
       "0               .NET, Azure, Kubernetes, Python, dbt                    15   \n",
       "1                 C#, Kafka, Node.js, Snowflake, dbt                    15   \n",
       "2   .NET, Node.js, React, Snowflake, TypeScript, dbt                     9   \n",
       "3   Airflow, C#, JavaScript, Kafka, Spark, Terraform                    15   \n",
       "4                    Airflow, Node.js, Python, Spark                    15   \n",
       "5                 Airflow, JavaScript, Power BI, dbt                    15   \n",
       "6          C#, Databricks, SQL, Snowflake, Terraform                    15   \n",
       "7  Airflow, JavaScript, Oracle, Power BI, SQL, Te...                    12   \n",
       "8       AWS, Azure, C#, Oracle, Power BI, TypeScript                    15   \n",
       "9             .NET, Airflow, Azure, React, Snowflake                    15   \n",
       "\n",
       "  clearance latest_availability_date  latest_percent_available  \n",
       "0        SC               2026-01-15                        40  \n",
       "1      None               2026-01-15                        78  \n",
       "2      None               2026-01-15                        33  \n",
       "3        SC               2026-01-15                        48  \n",
       "4        SC               2026-01-15                         0  \n",
       "5        SC               2026-01-15                        35  \n",
       "6      None               2026-01-15                        52  \n",
       "7      None               2026-01-15                        83  \n",
       "8        SC               2026-01-15                        23  \n",
       "9      None               2026-01-15                        24  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Step 6 — Refresh materialised search view\")\n",
    "\n",
    "from sqlalchemy import text\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    # simple (non-concurrent) refresh is fine for demo\n",
    "    conn.execute(text(\"REFRESH MATERIALIZED VIEW cv_search_profile_mv;\"))\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    mv_sample = pd.read_sql(\n",
    "        \"SELECT * FROM cv_search_profile_mv ORDER BY user_id LIMIT 10;\",\n",
    "        conn,\n",
    "    )\n",
    "\n",
    "print(\"\\nSample rows from cv_search_profile_mv:\")\n",
    "display(mv_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d34e6",
   "metadata": {},
   "source": [
    "## Step 6.1 - Example queries against cv_search_profile_mv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89c9ed76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Example queries against cv_search_profile_mv ====================\n",
      "\n",
      "Available SC (or above) candidates:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>cv_partner_user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>cv_id</th>\n",
       "      <th>cv_title</th>\n",
       "      <th>sfia_level</th>\n",
       "      <th>cpd_label</th>\n",
       "      <th>technologies</th>\n",
       "      <th>max_years_experience</th>\n",
       "      <th>clearance</th>\n",
       "      <th>latest_availability_date</th>\n",
       "      <th>latest_percent_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>442</td>\n",
       "      <td>c2d5c338</td>\n",
       "      <td>Albert Ballard</td>\n",
       "      <td>442</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>GCP, Kubernetes, Node.js, Snowflake, dbt</td>\n",
       "      <td>13</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>395</td>\n",
       "      <td>3aef8dc7</td>\n",
       "      <td>Andrea Reyes</td>\n",
       "      <td>395</td>\n",
       "      <td>Principal Databricks Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>GCP, Python, SQL</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>235</td>\n",
       "      <td>3eb64aca</td>\n",
       "      <td>Jasmine Brown</td>\n",
       "      <td>235</td>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>AWS, C#, Oracle</td>\n",
       "      <td>11</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119</td>\n",
       "      <td>dbe2732d</td>\n",
       "      <td>Pamela Sanchez</td>\n",
       "      <td>119</td>\n",
       "      <td>Senior AI Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>.NET, Databricks, Power BI, Snowflake, dbt</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256</td>\n",
       "      <td>60c36a34</td>\n",
       "      <td>Jason Murphy</td>\n",
       "      <td>256</td>\n",
       "      <td>Principal Full-stack Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>.NET, Azure, Kafka, Python, SQL</td>\n",
       "      <td>13</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>193</td>\n",
       "      <td>6378544e</td>\n",
       "      <td>Richard Phillips</td>\n",
       "      <td>193</td>\n",
       "      <td>Senior MLOps Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>Azure, Node.js, Snowflake</td>\n",
       "      <td>6</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>234</td>\n",
       "      <td>7e5d7715</td>\n",
       "      <td>Jesse Benson</td>\n",
       "      <td>234</td>\n",
       "      <td>Principal Solution Architect</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>Azure, JavaScript, Power BI, SQL, Terraform</td>\n",
       "      <td>11</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>231</td>\n",
       "      <td>97d89180</td>\n",
       "      <td>Chris Mitchell</td>\n",
       "      <td>231</td>\n",
       "      <td>Lead Data Platform Engineer</td>\n",
       "      <td>6</td>\n",
       "      <td>CPD4E</td>\n",
       "      <td>AWS, Azure, GCP, Kafka, TypeScript</td>\n",
       "      <td>12</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>136</td>\n",
       "      <td>a60df3d5</td>\n",
       "      <td>Lisa Evans</td>\n",
       "      <td>136</td>\n",
       "      <td>Senior Databricks Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>.NET, GCP, Node.js, Power BI, TypeScript</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>398</td>\n",
       "      <td>3c0b664e</td>\n",
       "      <td>Joshua Tyler</td>\n",
       "      <td>398</td>\n",
       "      <td>Senior Kubernetes Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>.NET, Airflow, Azure, React, Spark</td>\n",
       "      <td>11</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>451</td>\n",
       "      <td>a59236e4</td>\n",
       "      <td>Victoria Phillips</td>\n",
       "      <td>451</td>\n",
       "      <td>Senior Project Manager</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>Docker, Snowflake, Spark</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>25</td>\n",
       "      <td>b2ae8058</td>\n",
       "      <td>Cynthia Diaz</td>\n",
       "      <td>25</td>\n",
       "      <td>Lead Enterprise Architect</td>\n",
       "      <td>6</td>\n",
       "      <td>CPD4E</td>\n",
       "      <td>JavaScript, Kafka, Oracle, Spark, Terraform</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>467</td>\n",
       "      <td>c75b72d5</td>\n",
       "      <td>Gregory Anderson</td>\n",
       "      <td>467</td>\n",
       "      <td>Senior Analytics Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>Kafka, Kubernetes, Node.js, SQL</td>\n",
       "      <td>10</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>371</td>\n",
       "      <td>99ba9002</td>\n",
       "      <td>Elizabeth Hodge</td>\n",
       "      <td>371</td>\n",
       "      <td>Senior Analytics Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>.NET, AWS, Databricks, Power BI, dbt</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>155</td>\n",
       "      <td>baa47049</td>\n",
       "      <td>Laura Valencia</td>\n",
       "      <td>155</td>\n",
       "      <td>Principal Frontend Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>AWS, JavaScript, Node.js, Python, React, Terra...</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>397</td>\n",
       "      <td>e2c11d93</td>\n",
       "      <td>Natasha Shields</td>\n",
       "      <td>397</td>\n",
       "      <td>Principal Backend Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>Airflow, C#, Power BI, Spark, TypeScript</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>375</td>\n",
       "      <td>3d741e7a</td>\n",
       "      <td>Shawn Edwards</td>\n",
       "      <td>375</td>\n",
       "      <td>Senior Analytics Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>Airflow, Kubernetes, Power BI, Snowflake, Type...</td>\n",
       "      <td>14</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>109</td>\n",
       "      <td>91c1a26a</td>\n",
       "      <td>Angela Higgins</td>\n",
       "      <td>109</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>AWS, JavaScript, SQL</td>\n",
       "      <td>6</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>147</td>\n",
       "      <td>8766e68f</td>\n",
       "      <td>Dana Green</td>\n",
       "      <td>147</td>\n",
       "      <td>Head of Azure Engineering</td>\n",
       "      <td>6</td>\n",
       "      <td>CPD4E</td>\n",
       "      <td>Airflow, Databricks, Python, Snowflake, dbt</td>\n",
       "      <td>6</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>206</td>\n",
       "      <td>d6d1179d</td>\n",
       "      <td>Loretta Potter</td>\n",
       "      <td>206</td>\n",
       "      <td>Principal Data Scientist</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>JavaScript, Node.js, React, TypeScript</td>\n",
       "      <td>11</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id cv_partner_user_id          user_name  cv_id  \\\n",
       "0       442           c2d5c338     Albert Ballard    442   \n",
       "1       395           3aef8dc7       Andrea Reyes    395   \n",
       "2       235           3eb64aca      Jasmine Brown    235   \n",
       "3       119           dbe2732d     Pamela Sanchez    119   \n",
       "4       256           60c36a34       Jason Murphy    256   \n",
       "5       193           6378544e   Richard Phillips    193   \n",
       "6       234           7e5d7715       Jesse Benson    234   \n",
       "7       231           97d89180     Chris Mitchell    231   \n",
       "8       136           a60df3d5         Lisa Evans    136   \n",
       "9       398           3c0b664e       Joshua Tyler    398   \n",
       "10      451           a59236e4  Victoria Phillips    451   \n",
       "11       25           b2ae8058       Cynthia Diaz     25   \n",
       "12      467           c75b72d5   Gregory Anderson    467   \n",
       "13      371           99ba9002    Elizabeth Hodge    371   \n",
       "14      155           baa47049     Laura Valencia    155   \n",
       "15      397           e2c11d93    Natasha Shields    397   \n",
       "16      375           3d741e7a      Shawn Edwards    375   \n",
       "17      109           91c1a26a     Angela Higgins    109   \n",
       "18      147           8766e68f         Dana Green    147   \n",
       "19      206           d6d1179d     Loretta Potter    206   \n",
       "\n",
       "                         cv_title  sfia_level cpd_label  \\\n",
       "0            Senior Data Engineer           4     CPD3E   \n",
       "1   Principal Databricks Engineer           5     CPD3L   \n",
       "2         Principal Data Engineer           5     CPD3L   \n",
       "3              Senior AI Engineer           4     CPD3E   \n",
       "4   Principal Full-stack Engineer           5     CPD3L   \n",
       "5           Senior MLOps Engineer           4     CPD3E   \n",
       "6    Principal Solution Architect           5     CPD3L   \n",
       "7     Lead Data Platform Engineer           6     CPD4E   \n",
       "8      Senior Databricks Engineer           4     CPD3E   \n",
       "9      Senior Kubernetes Engineer           4     CPD3E   \n",
       "10         Senior Project Manager           4     CPD3E   \n",
       "11      Lead Enterprise Architect           6     CPD4E   \n",
       "12      Senior Analytics Engineer           4     CPD3E   \n",
       "13      Senior Analytics Engineer           4     CPD3E   \n",
       "14    Principal Frontend Engineer           5     CPD3L   \n",
       "15     Principal Backend Engineer           5     CPD3L   \n",
       "16      Senior Analytics Engineer           4     CPD3E   \n",
       "17           Senior Data Engineer           4     CPD3E   \n",
       "18      Head of Azure Engineering           6     CPD4E   \n",
       "19       Principal Data Scientist           5     CPD3L   \n",
       "\n",
       "                                         technologies  max_years_experience  \\\n",
       "0            GCP, Kubernetes, Node.js, Snowflake, dbt                    13   \n",
       "1                                    GCP, Python, SQL                    14   \n",
       "2                                     AWS, C#, Oracle                    11   \n",
       "3          .NET, Databricks, Power BI, Snowflake, dbt                    14   \n",
       "4                     .NET, Azure, Kafka, Python, SQL                    13   \n",
       "5                           Azure, Node.js, Snowflake                     6   \n",
       "6         Azure, JavaScript, Power BI, SQL, Terraform                    11   \n",
       "7                  AWS, Azure, GCP, Kafka, TypeScript                    12   \n",
       "8            .NET, GCP, Node.js, Power BI, TypeScript                    15   \n",
       "9                  .NET, Airflow, Azure, React, Spark                    11   \n",
       "10                           Docker, Snowflake, Spark                    14   \n",
       "11        JavaScript, Kafka, Oracle, Spark, Terraform                    14   \n",
       "12                    Kafka, Kubernetes, Node.js, SQL                    10   \n",
       "13               .NET, AWS, Databricks, Power BI, dbt                    14   \n",
       "14  AWS, JavaScript, Node.js, Python, React, Terra...                    14   \n",
       "15           Airflow, C#, Power BI, Spark, TypeScript                    14   \n",
       "16  Airflow, Kubernetes, Power BI, Snowflake, Type...                    14   \n",
       "17                               AWS, JavaScript, SQL                     6   \n",
       "18        Airflow, Databricks, Python, Snowflake, dbt                     6   \n",
       "19             JavaScript, Node.js, React, TypeScript                    11   \n",
       "\n",
       "   clearance latest_availability_date  latest_percent_available  \n",
       "0         SC               2026-01-15                        96  \n",
       "1         SC               2026-01-15                        93  \n",
       "2         SC               2026-01-15                        90  \n",
       "3         SC               2026-01-15                        89  \n",
       "4         SC               2026-01-15                        86  \n",
       "5         SC               2026-01-15                        86  \n",
       "6         SC               2026-01-15                        84  \n",
       "7         SC               2026-01-15                        83  \n",
       "8         SC               2026-01-15                        83  \n",
       "9         SC               2026-01-15                        83  \n",
       "10        SC               2026-01-15                        83  \n",
       "11        SC               2026-01-15                        80  \n",
       "12        SC               2026-01-15                        79  \n",
       "13        SC               2026-01-15                        79  \n",
       "14        SC               2026-01-15                        77  \n",
       "15        SC               2026-01-15                        76  \n",
       "16        SC               2026-01-15                        76  \n",
       "17        SC               2026-01-15                        75  \n",
       "18        SC               2026-01-15                        74  \n",
       "19        SC               2026-01-15                        73  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Example queries against cv_search_profile_mv\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # e.g. available people with SC clearance and SFIA >= 4\n",
    "    sc_available = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT *\n",
    "        FROM cv_search_profile_mv\n",
    "        WHERE\n",
    "            (clearance = 'SC' OR clearance IS NULL) AND\n",
    "            sfia_level >= 4 AND\n",
    "            latest_percent_available >= 50\n",
    "        ORDER BY latest_percent_available DESC, sfia_level DESC\n",
    "        LIMIT 20;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "\n",
    "print(\"\\nAvailable SC (or above) candidates:\")\n",
    "display(sc_available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c43c5",
   "metadata": {},
   "source": [
    "# Step 7 — Explore Validations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8e635",
   "metadata": {},
   "source": [
    "## 7.1 Pick a random user + CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ae51c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 7.1 — Pick one random user + CV ====================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>cv_partner_user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>department</th>\n",
       "      <th>country</th>\n",
       "      <th>cv_id</th>\n",
       "      <th>cv_partner_cv_id</th>\n",
       "      <th>cv_title</th>\n",
       "      <th>sfia_level</th>\n",
       "      <th>cpd_level</th>\n",
       "      <th>cpd_band</th>\n",
       "      <th>cpd_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57</td>\n",
       "      <td>db3a4874</td>\n",
       "      <td>Vincent White</td>\n",
       "      <td>vincent.white@mail.test</td>\n",
       "      <td>Data Engineering</td>\n",
       "      <td>USA</td>\n",
       "      <td>57</td>\n",
       "      <td>cv_db3a4874</td>\n",
       "      <td>Consultant Python Developer</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>L</td>\n",
       "      <td>CPD2L</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id cv_partner_user_id           name                    email  \\\n",
       "0       57           db3a4874  Vincent White  vincent.white@mail.test   \n",
       "\n",
       "         department country  cv_id cv_partner_cv_id  \\\n",
       "0  Data Engineering     USA     57      cv_db3a4874   \n",
       "\n",
       "                      cv_title  sfia_level  cpd_level cpd_band cpd_label  \n",
       "0  Consultant Python Developer           3          2        L     CPD2L  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chosen user_id=57, cv_id=57\n"
     ]
    }
   ],
   "source": [
    "print_step(\"Step 7.1 — Pick one random user + CV\")\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    person_df = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            u.user_id,\n",
    "            u.cv_partner_user_id,\n",
    "            (u.name_multilang->>'int') AS name,\n",
    "            u.email,\n",
    "            u.department,\n",
    "            u.country,\n",
    "            c.cv_id,\n",
    "            c.cv_partner_cv_id,\n",
    "            (c.title_multilang->>'int') AS cv_title,\n",
    "            c.sfia_level,\n",
    "            c.cpd_level,\n",
    "            c.cpd_band,\n",
    "            c.cpd_label\n",
    "        FROM users u\n",
    "        JOIN cvs c ON c.user_id = u.user_id\n",
    "        ORDER BY random()\n",
    "        LIMIT 1;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "\n",
    "display(person_df)\n",
    "\n",
    "user_id = int(person_df.loc[0, \"user_id\"])\n",
    "cv_id = int(person_df.loc[0, \"cv_id\"])\n",
    "\n",
    "print(f\"\\nChosen user_id={user_id}, cv_id={cv_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347d40c",
   "metadata": {},
   "source": [
    "## 7.2 Pull all related sections for that CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95506d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 7.2 — Load all sections for this CV ====================\n",
      "\n",
      "Technologies:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>technology</th>\n",
       "      <th>years_experience</th>\n",
       "      <th>proficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWS</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Power BI</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terraform</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  technology  years_experience  proficiency\n",
       "0        AWS                 4            2\n",
       "1   Power BI                12            2\n",
       "2  Terraform                 2            3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Languages:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>level</th>\n",
       "      <th>highlighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Norwegian</td>\n",
       "      <td>Fluent</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>English</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Polish</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    language         level  highlighted\n",
       "0  Norwegian        Fluent         True\n",
       "1    English  Intermediate        False\n",
       "2     Polish  Intermediate        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Project experience:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_from</th>\n",
       "      <th>year_to</th>\n",
       "      <th>description</th>\n",
       "      <th>industry</th>\n",
       "      <th>project_type</th>\n",
       "      <th>percent_allocated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year_from, year_to, description, industry, project_type, percent_allocated]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Work experience:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_from</th>\n",
       "      <th>year_to</th>\n",
       "      <th>employer</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017</td>\n",
       "      <td>2021</td>\n",
       "      <td>HealthCorp</td>\n",
       "      <td>Worked on data platforms, software delivery an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>2017</td>\n",
       "      <td>HealthCorp</td>\n",
       "      <td>Worked on data platforms, software delivery an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_from  year_to    employer  \\\n",
       "0       2017     2021  HealthCorp   \n",
       "1       2015     2017  HealthCorp   \n",
       "\n",
       "                                         description  \n",
       "0  Worked on data platforms, software delivery an...  \n",
       "1  Worked on data platforms, software delivery an...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Education:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_from</th>\n",
       "      <th>year_to</th>\n",
       "      <th>place_of_study</th>\n",
       "      <th>degree</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>2012</td>\n",
       "      <td>NTNU</td>\n",
       "      <td>BEng Software Eng</td>\n",
       "      <td>Thesis on scalable data pipelines.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>2013</td>\n",
       "      <td>UCL</td>\n",
       "      <td>BEng Software Eng</td>\n",
       "      <td>Thesis on scalable data pipelines.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_from  year_to place_of_study             degree  \\\n",
       "0       2009     2012           NTNU  BEng Software Eng   \n",
       "1       2008     2013            UCL  BEng Software Eng   \n",
       "\n",
       "                          description  \n",
       "0  Thesis on scalable data pipelines.  \n",
       "1  Thesis on scalable data pipelines.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Courses:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>organiser</th>\n",
       "      <th>highlighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [year, name, organiser, highlighted]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Certifications:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_expire</th>\n",
       "      <th>year_expire</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  month  month_expire  year_expire\n",
       "0  2022     10            10         2023\n",
       "1  2019      6             6         2022"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Positions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_from</th>\n",
       "      <th>year_to</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>highlighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>2020</td>\n",
       "      <td>Consultant Python Developer</td>\n",
       "      <td>Progression based on delivery impact.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>2019</td>\n",
       "      <td>Associate Python Developer</td>\n",
       "      <td>Progression based on delivery impact.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year_from  year_to                         name  \\\n",
       "0       2019     2020  Consultant Python Developer   \n",
       "1       2018     2019   Associate Python Developer   \n",
       "\n",
       "                             description  highlighted  \n",
       "0  Progression based on delivery impact.        False  \n",
       "1  Progression based on delivery impact.        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Blogs / publications:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>highlighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLOps Playbook</td>\n",
       "      <td>Conference talk / blog summary.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name                      description  highlighted\n",
       "0  MLOps Playbook  Conference talk / blog summary.         True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV roles:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>highlighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architect</td>\n",
       "      <td>High-level role on multiple projects.</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Developer</td>\n",
       "      <td>High-level role on multiple projects.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name                            description  highlighted\n",
       "0  Architect  High-level role on multiple projects.         True\n",
       "1  Developer  High-level role on multiple projects.        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key qualifications:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>summary</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Key Strengths</td>\n",
       "      <td>Experienced in cloud, data engineering and ana...</td>\n",
       "      <td>Focus on Python, Azure/AWS, Databricks.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label                                            summary  \\\n",
       "0  Key Strengths  Experienced in cloud, data engineering and ana...   \n",
       "\n",
       "                         short_description  \n",
       "0  Focus on Python, Azure/AWS, Databricks.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Step 7.2 — Load all sections for this CV\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Technologies\n",
    "    techs = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          dt.name AS technology,\n",
    "          ct.years_experience,\n",
    "          ct.proficiency\n",
    "        FROM cv_technology ct\n",
    "        JOIN dim_technology dt ON dt.technology_id = ct.technology_id\n",
    "        WHERE ct.cv_id = %(cv_id)s\n",
    "        ORDER BY dt.name;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Languages\n",
    "    langs = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          dl.name AS language,\n",
    "          cl.level,\n",
    "          cl.highlighted\n",
    "        FROM cv_language cl\n",
    "        JOIN dim_language dl ON dl.language_id = cl.language_id\n",
    "        WHERE cl.cv_id = %(cv_id)s\n",
    "        ORDER BY cl.highlighted DESC, dl.name;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Project experience\n",
    "    projects = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          pe.year_from, pe.year_to,\n",
    "          (pe.description_multilang->>'int') AS description,\n",
    "          di.name AS industry,\n",
    "          dpt.name AS project_type,\n",
    "          pe.percent_allocated\n",
    "        FROM project_experience pe\n",
    "        LEFT JOIN dim_industry di ON di.industry_id = pe.industry_id\n",
    "        LEFT JOIN dim_project_type dpt ON dpt.project_type_id = pe.project_type_id\n",
    "        WHERE pe.cv_id = %(cv_id)s\n",
    "        ORDER BY pe.year_from DESC, pe.month_from DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Work experience\n",
    "    work = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          year_from, year_to,\n",
    "          employer,\n",
    "          description\n",
    "        FROM work_experience\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY year_from DESC, month_from DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Education\n",
    "    edu = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          year_from, year_to,\n",
    "          place_of_study,\n",
    "          degree,\n",
    "          description\n",
    "        FROM education\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY year_from DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Courses\n",
    "    courses = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          year,\n",
    "          name,\n",
    "          organiser,\n",
    "          highlighted\n",
    "        FROM course\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY year DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Certifications\n",
    "    certs = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          year,\n",
    "          month,\n",
    "          month_expire,\n",
    "          year_expire\n",
    "        FROM certification\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY year DESC, month DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Positions\n",
    "    positions = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          year_from, year_to,\n",
    "          name,\n",
    "          description,\n",
    "          highlighted\n",
    "        FROM position\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY year_from DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Blogs / publications\n",
    "    blogs = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          name,\n",
    "          description,\n",
    "          highlighted\n",
    "        FROM blog_publication\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY highlighted DESC, updated DESC NULLS LAST;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # CV roles\n",
    "    roles = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          name,\n",
    "          description,\n",
    "          highlighted\n",
    "        FROM cv_role\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY highlighted DESC, name;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "    # Key qualifications\n",
    "    key_quals = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          label,\n",
    "          summary,\n",
    "          short_description\n",
    "        FROM key_qualification\n",
    "        WHERE cv_id = %(cv_id)s\n",
    "        ORDER BY label;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"cv_id\": cv_id},\n",
    "    )\n",
    "\n",
    "print(\"\\nTechnologies:\")\n",
    "display(techs.head())\n",
    "\n",
    "print(\"\\nLanguages:\")\n",
    "display(langs.head())\n",
    "\n",
    "print(\"\\nProject experience:\")\n",
    "display(projects.head())\n",
    "\n",
    "print(\"\\nWork experience:\")\n",
    "display(work.head())\n",
    "\n",
    "print(\"\\nEducation:\")\n",
    "display(edu.head())\n",
    "\n",
    "print(\"\\nCourses:\")\n",
    "display(courses.head())\n",
    "\n",
    "print(\"\\nCertifications:\")\n",
    "display(certs.head())\n",
    "\n",
    "print(\"\\nPositions:\")\n",
    "display(positions.head())\n",
    "\n",
    "print(\"\\nBlogs / publications:\")\n",
    "display(blogs.head())\n",
    "\n",
    "print(\"\\nCV roles:\")\n",
    "display(roles.head())\n",
    "\n",
    "print(\"\\nKey qualifications:\")\n",
    "display(key_quals.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5572e634",
   "metadata": {},
   "source": [
    "## 7.3 Clearance + availability for that user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1e1bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Step 7.3 — Clearance and availability for this user ====================\n",
      "\n",
      "Security clearance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clearance</th>\n",
       "      <th>valid_from</th>\n",
       "      <th>valid_to</th>\n",
       "      <th>verified_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SC</td>\n",
       "      <td>2024-10-10</td>\n",
       "      <td>None</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>1900-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clearance  valid_from valid_to verified_by\n",
       "0        SC  2024-10-10     None          HR\n",
       "1      None  1900-01-01     None          HR"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Upcoming availability (sample):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>percent_available</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-15</td>\n",
       "      <td>100</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-16</td>\n",
       "      <td>100</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-18</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-19</td>\n",
       "      <td>0</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-20</td>\n",
       "      <td>51</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-21</td>\n",
       "      <td>59</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2025-11-22</td>\n",
       "      <td>100</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2025-11-23</td>\n",
       "      <td>100</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2025-11-24</td>\n",
       "      <td>35</td>\n",
       "      <td>Fake generator</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  percent_available          source\n",
       "0  2025-11-15                100  Fake generator\n",
       "1  2025-11-16                100  Fake generator\n",
       "2  2025-11-17                  0  Fake generator\n",
       "3  2025-11-18                  0  Fake generator\n",
       "4  2025-11-19                  0  Fake generator\n",
       "5  2025-11-20                 51  Fake generator\n",
       "6  2025-11-21                 59  Fake generator\n",
       "7  2025-11-22                100  Fake generator\n",
       "8  2025-11-23                100  Fake generator\n",
       "9  2025-11-24                 35  Fake generator"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Step 7.3 — Clearance and availability for this user\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    clearance = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          dc.name AS clearance,\n",
    "          uc.valid_from,\n",
    "          uc.valid_to,\n",
    "          uc.verified_by\n",
    "        FROM user_clearance uc\n",
    "        JOIN dim_clearance dc ON dc.clearance_id = uc.clearance_id\n",
    "        WHERE uc.user_id = %(user_id)s\n",
    "        ORDER BY uc.valid_from DESC;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"user_id\": user_id},\n",
    "    )\n",
    "\n",
    "    availability = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "          date,\n",
    "          percent_available,\n",
    "          source\n",
    "        FROM user_availability\n",
    "        WHERE user_id = %(user_id)s\n",
    "        ORDER BY date\n",
    "        LIMIT 30;\n",
    "        \"\"\",\n",
    "        conn,\n",
    "        params={\"user_id\": user_id},\n",
    "    )\n",
    "\n",
    "print(\"\\nSecurity clearance:\")\n",
    "display(clearance)\n",
    "\n",
    "print(\"\\nUpcoming availability (sample):\")\n",
    "display(availability.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c5ba883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Example table use ====================\n",
      "\n",
      "One-row-per-person profile table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>cv_partner_user_id</th>\n",
       "      <th>name</th>\n",
       "      <th>cv_id</th>\n",
       "      <th>cv_title</th>\n",
       "      <th>sfia_level</th>\n",
       "      <th>cpd_label</th>\n",
       "      <th>technologies</th>\n",
       "      <th>max_years_experience</th>\n",
       "      <th>clearance</th>\n",
       "      <th>latest_availability_date</th>\n",
       "      <th>latest_percent_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b89ca31a</td>\n",
       "      <td>Danielle Johnson</td>\n",
       "      <td>1</td>\n",
       "      <td>Principal C# Developer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>.NET, Azure, Kubernetes, Python, dbt</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>74b836a3</td>\n",
       "      <td>Joshua Walker</td>\n",
       "      <td>2</td>\n",
       "      <td>Principal Data Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>C#, Kafka, Node.js, Snowflake, dbt</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70aaa8ab</td>\n",
       "      <td>Jill Rhodes</td>\n",
       "      <td>3</td>\n",
       "      <td>Senior C# Developer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>.NET, Node.js, React, Snowflake, TypeScript, dbt</td>\n",
       "      <td>9</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>080ef439</td>\n",
       "      <td>Patricia Miller</td>\n",
       "      <td>4</td>\n",
       "      <td>Principal Analytics Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>Airflow, C#, JavaScript, Kafka, Spark, Terraform</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4253cf51</td>\n",
       "      <td>Robert Johnson</td>\n",
       "      <td>5</td>\n",
       "      <td>Associate ML Engineer</td>\n",
       "      <td>2</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>Airflow, Node.js, Python, Spark</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6b7c67d6</td>\n",
       "      <td>Jeffery Wagner</td>\n",
       "      <td>6</td>\n",
       "      <td>Consultant Data Platform Engineer</td>\n",
       "      <td>3</td>\n",
       "      <td>CPD2L</td>\n",
       "      <td>Airflow, JavaScript, Power BI, dbt</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1266d379</td>\n",
       "      <td>Anthony Gonzalez</td>\n",
       "      <td>7</td>\n",
       "      <td>Principal Azure Engineer</td>\n",
       "      <td>5</td>\n",
       "      <td>CPD3L</td>\n",
       "      <td>C#, Databricks, SQL, Snowflake, Terraform</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>f81d901a</td>\n",
       "      <td>Debra Gardner</td>\n",
       "      <td>8</td>\n",
       "      <td>Senior AI Engineer</td>\n",
       "      <td>4</td>\n",
       "      <td>CPD3E</td>\n",
       "      <td>Airflow, JavaScript, Oracle, Power BI, SQL, Te...</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>27b5fdea</td>\n",
       "      <td>Jeffrey Lawrence</td>\n",
       "      <td>9</td>\n",
       "      <td>Associate Backend Engineer</td>\n",
       "      <td>2</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>AWS, Azure, C#, Oracle, Power BI, TypeScript</td>\n",
       "      <td>15</td>\n",
       "      <td>SC</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8eaaedc9</td>\n",
       "      <td>Lisa Smith</td>\n",
       "      <td>10</td>\n",
       "      <td>Associate Data Scientist</td>\n",
       "      <td>2</td>\n",
       "      <td>CPD1E</td>\n",
       "      <td>.NET, Airflow, Azure, React, Snowflake</td>\n",
       "      <td>15</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id cv_partner_user_id              name  cv_id  \\\n",
       "0        1           b89ca31a  Danielle Johnson      1   \n",
       "1        2           74b836a3     Joshua Walker      2   \n",
       "2        3           70aaa8ab       Jill Rhodes      3   \n",
       "3        4           080ef439   Patricia Miller      4   \n",
       "4        5           4253cf51    Robert Johnson      5   \n",
       "5        6           6b7c67d6    Jeffery Wagner      6   \n",
       "6        7           1266d379  Anthony Gonzalez      7   \n",
       "7        8           f81d901a     Debra Gardner      8   \n",
       "8        9           27b5fdea  Jeffrey Lawrence      9   \n",
       "9       10           8eaaedc9        Lisa Smith     10   \n",
       "\n",
       "                            cv_title  sfia_level cpd_label  \\\n",
       "0             Principal C# Developer           5     CPD3L   \n",
       "1            Principal Data Engineer           5     CPD3L   \n",
       "2                Senior C# Developer           4     CPD3E   \n",
       "3       Principal Analytics Engineer           5     CPD3L   \n",
       "4              Associate ML Engineer           2     CPD1E   \n",
       "5  Consultant Data Platform Engineer           3     CPD2L   \n",
       "6           Principal Azure Engineer           5     CPD3L   \n",
       "7                 Senior AI Engineer           4     CPD3E   \n",
       "8         Associate Backend Engineer           2     CPD1E   \n",
       "9           Associate Data Scientist           2     CPD1E   \n",
       "\n",
       "                                        technologies  max_years_experience  \\\n",
       "0               .NET, Azure, Kubernetes, Python, dbt                    15   \n",
       "1                 C#, Kafka, Node.js, Snowflake, dbt                    15   \n",
       "2   .NET, Node.js, React, Snowflake, TypeScript, dbt                     9   \n",
       "3   Airflow, C#, JavaScript, Kafka, Spark, Terraform                    15   \n",
       "4                    Airflow, Node.js, Python, Spark                    15   \n",
       "5                 Airflow, JavaScript, Power BI, dbt                    15   \n",
       "6          C#, Databricks, SQL, Snowflake, Terraform                    15   \n",
       "7  Airflow, JavaScript, Oracle, Power BI, SQL, Te...                    12   \n",
       "8       AWS, Azure, C#, Oracle, Power BI, TypeScript                    15   \n",
       "9             .NET, Airflow, Azure, React, Snowflake                    15   \n",
       "\n",
       "  clearance latest_availability_date  latest_percent_available  \n",
       "0        SC               2026-01-15                       100  \n",
       "1      None               2026-01-15                       100  \n",
       "2      None               2026-01-15                       100  \n",
       "3        SC               2026-01-15                       100  \n",
       "4        SC               2026-01-15                       100  \n",
       "5        SC               2026-01-15                       100  \n",
       "6      None               2026-01-15                       100  \n",
       "7      None               2026-01-15                       100  \n",
       "8        SC               2026-01-15                       100  \n",
       "9      None               2026-01-15                       100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_step(\"Example table use\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    basic = pd.read_sql(\"\"\"\n",
    "        SELECT\n",
    "            u.user_id,\n",
    "            u.cv_partner_user_id,\n",
    "            (u.name_multilang->>'int') AS name,\n",
    "            c.cv_id,\n",
    "            (c.title_multilang->>'int') AS cv_title,\n",
    "            c.sfia_level,\n",
    "            c.cpd_label,\n",
    "            dt.name AS technology,\n",
    "            ct.years_experience,\n",
    "            cr.name AS clearance,\n",
    "            ua.percent_available, \n",
    "            ua.date AS availability_date\n",
    "        FROM users u\n",
    "        JOIN cvs c\n",
    "            ON c.user_id = u.user_id\n",
    "        LEFT JOIN cv_technology ct  ON ct.cv_id = c.cv_id\n",
    "        LEFT JOIN dim_technology dt ON dt.technology_id = ct.technology_id\n",
    "        LEFT JOIN user_clearance uc ON uc.user_id = u.user_id\n",
    "        LEFT JOIN dim_clearance cr  ON cr.clearance_id = uc.clearance_id\n",
    "        LEFT JOIN user_availability ua ON ua.user_id = u.user_id\n",
    "\n",
    "    \"\"\", conn)\n",
    "\n",
    "basic_clean = (\n",
    "    basic.sort_values(\n",
    "        by=[\"user_id\", \"cv_id\", \"technology\", \"availability_date\", \"clearance\"],\n",
    "        ascending=[True, True, True, True, False]  # SC before None, etc.\n",
    "    )\n",
    "    .drop_duplicates(\n",
    "        subset=[\"user_id\", \"cv_id\", \"technology\", \"availability_date\"],\n",
    "        keep=\"first\"\n",
    "    )\n",
    ")\n",
    "\n",
    "profile = (\n",
    "    basic_clean\n",
    "    .groupby(\n",
    "        [\n",
    "            \"user_id\",\n",
    "            \"cv_partner_user_id\",\n",
    "            \"name\",\n",
    "            \"cv_id\",\n",
    "            \"cv_title\",\n",
    "            \"sfia_level\",\n",
    "            \"cpd_label\",\n",
    "        ],\n",
    "        dropna=False,\n",
    "    )\n",
    "    .agg(\n",
    "        technologies=(\"technology\", lambda s: \", \".join(sorted(set(s.dropna())))),\n",
    "        max_years_experience=(\"years_experience\", \"max\"),\n",
    "        clearance=(\"clearance\", lambda s: s.dropna().iloc[0] if s.dropna().any() else None),\n",
    "        latest_availability_date=(\"availability_date\", \"max\"),\n",
    "        latest_percent_available=(\"percent_available\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"\\nOne-row-per-person profile table:\")\n",
    "display(profile.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
